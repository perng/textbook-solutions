\setcounter{chapter}{5}
\chapter{Functions of Bounded Variation and Rectifiable Curves}

\section{Functions of bounded variation}

\begin{definitionssection}{Definitions and Theorems}
\end{definitionssection}

\begin{definition}[Bounded Variation]
A function $f$ defined on an interval $[a, b]$ is said to be of bounded variation if there exists a positive number $M$ such that for every partition $P = \{x_0, x_1, \ldots, x_n\}$ of $[a, b]$, the sum $\sum_{k=1}^n |f(x_k) - f(x_{k-1})| \leq M$. The supremum of all such sums is called the total variation of $f$ on $[a, b]$ and is denoted by $V_f(a, b)$.
\end{definition}

\begin{importance}
\noindent\textbf{Importance:} Bounded variation is a fundamental concept that measures how much a function oscillates. It provides a bridge between continuous functions and differentiable functions, and is essential for understanding the Riemann-Stieltjes integral. Functions of bounded variation have many nice properties, including being differentiable almost everywhere and having a well-defined total variation.
\end{importance}



\begin{definition}[Rectifiable Curve]
A curve $\gamma$ in $\mathbb{R}^n$ is called rectifiable if it has finite length. The length of a rectifiable curve is defined as the supremum of the lengths of all inscribed polygonal approximations.
\end{definition}

\begin{importance}
\noindent\textbf{Importance:} Rectifiable curves are the geometric counterpart of functions of bounded variation. They provide a rigorous foundation for measuring curve lengths and are essential for understanding geometric properties of curves in higher dimensions. This concept is crucial for differential geometry and calculus on manifolds.
\end{importance}



\begin{theorem}[Jordan Decomposition]
Every function of bounded variation on $[a, b]$ can be written as the difference of two increasing functions: $f = g - h$, where $g$ and $h$ are increasing functions on $[a, b]$.
\end{theorem}

\begin{importance}
\noindent\textbf{Importance:} This is one of the most fundamental results about functions of bounded variation. It reduces the study of bounded variation functions to the study of monotone functions, which are much simpler to understand. This decomposition is essential for proving many properties of bounded variation functions and for integration theory.
\end{importance}



\begin{theorem}[Differentiability of Bounded Variation Functions]
If $f$ is of bounded variation on $[a, b]$, then $f$ is differentiable almost everywhere on $[a, b]$.
\end{theorem}

\begin{importance}
\noindent\textbf{Importance:} This result shows that bounded variation functions are "almost" differentiable, which makes them much more regular than arbitrary continuous functions. This property is crucial for integration theory and for understanding the relationship between differentiability and integrability.
\end{importance}



\begin{theorem}[Total Variation Formula]
If $f$ is absolutely continuous on $[a, b]$, then $V_f(a, b) = \int_a^b |f'(x)| \, dx$.
\end{theorem}

\begin{importance}
\noindent\textbf{Importance:} This formula provides a practical way to compute the total variation of absolutely continuous functions using their derivatives. It connects the geometric concept of variation with the analytical concept of the derivative. While the integral formula is useful for computation, the fundamental idea is that the total variation measures how much the function "wiggles" and can be understood through the behavior of the derivative.
\end{importance}




\begin{problembox}[6.1: Functions of Bounded Variation]
\begin{problemstatement}
Determine which of the following functions are of bounded variation on $[0, 1]$.

\begin{enumerate}[label=(\alph*)]
\item $f(x) = x^2 \sin (1/x)$ if $x \neq 0$, $f(0) = 0$.

\item $f(x) = \sqrt{x} \sin (1/x)$ if $x \neq 0$, $f(0) = 0$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For (a), use the definition of bounded variation directly by showing that any partition has bounded total variation. For (b), construct a specific partition where the function oscillates to show the total variation is infinite.

\bigskip\noindent\textbf{Solution:}
\,(a) Let $f(x) = x^2 \sin(1/x)$ for $x \neq 0$ and $f(0) = 0$. We will show that \fbox{$f$ is of bounded variation on $[0,1]$} by bounding the total variation of any partition.

First, note that $|f(x)| \leq x^2$ for all $x \in [0,1]$, so $f$ is bounded. For any partition $P = \{0 = x_0 < x_1 < \cdots < x_n = 1\}$, we have:
\[\sum_{i=1}^n |f(x_i) - f(x_{i-1})| \leq \sum_{i=1}^n (|f(x_i)| + |f(x_{i-1})|) \leq 2 \sum_{i=1}^n x_i^2 \leq 2 \sum_{i=1}^n 1 = 2n.\]

However, this bound depends on the number of partition points. To get a uniform bound, we need a more careful analysis.
 
For $x \in (0,1]$, we have $f'(x) = 2x\sin(1/x) - \cos(1/x)$. Since $|2x\sin(1/x)| \leq 2x$ and $|\cos(1/x)| \leq 1$, we get $|f'(x)| \leq 2x + 1 \leq 3$ for all $x \in (0,1]$.

Now, for any partition $P = \{0 = x_0 < x_1 < \cdots < x_n = 1\}$, by the Mean Value Theorem, for each $i$ there exists $\xi_i \in (x_{i-1}, x_i)$ such that:
\[|f(x_i) - f(x_{i-1})| = |f'(\xi_i)| \cdot |x_i - x_{i-1}| \leq 3|x_i - x_{i-1}|.\]

Therefore:
\[\sum_{i=1}^n |f(x_i) - f(x_{i-1})| \leq 3 \sum_{i=1}^n |x_i - x_{i-1}| = 3(1-0) = 3.\]

This shows that the total variation is bounded above by 3 for any partition, so $f$ is of bounded variation on $[0,1]$.

\,(b) Let $f(x) = \sqrt{x} \sin(1/x)$ for $x \neq 0$ and $f(0) = 0$. We will construct a specific partition to show that \fbox{the total variation is infinite}.

Let $a_n = \dfrac{1}{(n+\tfrac12)\pi}$ for $n \geq 1$. Then:
\[f(a_n) = \sqrt{a_n} \sin((n+\tfrac12)\pi) = \sqrt{a_n} \cdot (-1)^n.\]

Consider the partition $P = \{0, a_1, a_2, \ldots, a_N, 1\}$ where $N$ is large. The total variation satisfies:
\[V_f(0,1) \geq \sum_{n=1}^{N-1} |f(a_{n+1}) - f(a_n)|.\]

Since $f(a_n) = (-1)^n \sqrt{a_n}$, we have:
\[|f(a_{n+1}) - f(a_n)| = |(-1)^{n+1} \sqrt{a_{n+1}} - (-1)^n \sqrt{a_n}| = \sqrt{a_{n+1}} + \sqrt{a_n}.\]

Therefore:
\[V_f(0,1) \geq \sum_{n=1}^{N-1} (\sqrt{a_{n+1}} + \sqrt{a_n}) \geq \sum_{n=1}^{N-1} \sqrt{a_n}.\]

Since $a_n = \dfrac{1}{(n+\tfrac12)\pi} \asymp \dfrac{1}{n}$ for large $n$, we have $\sqrt{a_n} \asymp \dfrac{1}{\sqrt{n}}$. The series $\sum_{n=1}^{\infty} \dfrac{1}{\sqrt{n}}$ diverges, so as $N \to \infty$, the total variation becomes infinite.

Thus $f(x) = \sqrt{x} \sin(1/x)$ is not of bounded variation on $[0,1]$.

\textbf{Observation:} The key difference between these two functions lies in how their oscillations are controlled. For $f(x) = x^2 \sin(1/x)$, the $x^2$ factor dampens the oscillations as $x \to 0$, ensuring that even though the sine function oscillates infinitely many times, the amplitude of these oscillations decreases rapidly enough to keep the total variation finite. In contrast, for $f(x) = \sqrt{x} \sin(1/x)$, the $\sqrt{x}$ factor decreases too slowly to control the oscillations effectively, allowing the total variation to accumulate to infinity.\qed


\begin{problembox}[6.2: Uniform Lipschitz Condition]
\begin{problemstatement}
A function $f$, defined on $[a, b]$, is said to satisfy a uniform Lipschitz condition of order $\alpha > 0$ on $[a, b]$ if there exists a constant $M > 0$ such that $|f(x) - f(y)| < M |x - y|^\alpha$ for all $x$ and $y$ in $[a, b]$. (Compare with Exercise 5.1.)

\begin{enumerate}[label=(\alph*)]
\item If $f$ is such a function, show that $\alpha > 1$ implies $f$ is constant on $[a, b]$, whereas $\alpha = 1$ implies $f$ is of bounded variation on $[a, b]$.

\item Give an example of a function $f$ satisfying a uniform Lipschitz condition of order $\alpha < 1$ on $[a, b]$ such that $f$ is not of bounded variation on $[a, b]$.

\item Give an example of a function $f$ which is of bounded variation on $[a, b]$ but which satisfies no uniform Lipschitz condition on $[a, b]$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For (a), use subdivision to show that when $\alpha > 1$, the function must be constant, and when $\alpha = 1$, it's Lipschitz and hence absolutely continuous. For (b), use a Weierstrass-type function with appropriate scaling. For (c), use a step function which has bounded variation but is discontinuous.

\bigskip\noindent\textbf{Solution:}
\,(a) If $\alpha>1$, subdivide $[x,y]$ into $n$ equal parts: then
\[|f(y)-f(x)|\le n\,M\Big(\tfrac{y-x}{n}\Big)^{\!\alpha}=M(y-x)^{\alpha}\,n^{1-\alpha}\to 0\ (n\to\infty),\]
so $f(y)=f(x)$ for all $x<y$ and $f$ is constant. If $\alpha=1$, the estimate $|f(x)-f(y)|\le M|x-y|$ shows $f$ is Lipschitz; hence $f$ is absolutely continuous and has $V_f(a,b)\le M(b-a)$.

\,(b) For $0<\alpha<1$, a standard example is the Weierstrass-type series on $[0,2\pi]$:
\[f(x)=\sum_{k=0}^{\infty}2^{-k\alpha}\sin(2^k x).\]
One checks (by splitting frequencies at the dyadic scale $2^k\approx |x-y|^{-1}$) that $|f(x)-f(y)|\le C|x-y|^{\alpha}$ for some $C$. Moreover, the total variation of the $N$th partial sum satisfies $V\big(\sum_{k=0}^N2^{-k\alpha}\sin(2^k x)\big)\ge c\sum_{k=0}^N2^{k(1-\alpha)}\to\infty$, so $f$ is not of bounded variation.

\,(c) Let $f$ be the step function $f(x)=\mathbf{1}_{[c,b]}(x)$ for some $c\in(a,b)$. Then $f$ has bounded variation $V_f(a,b)=1$ but is discontinuous, hence it satisfies no uniform Lipschitz condition on $[a,b]$.\qed


\begin{problembox}[6.3: Polynomials and Bounded Variation]
\begin{problemstatement}
Show that a polynomial $f$ is of bounded variation on every compact interval $[a, b]$. Describe a method for finding the total variation of $f$ on $[a, b]$ if the zeros of the derivative $f'$ are known.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the fact that polynomials are continuously differentiable (have continuous derivatives), hence absolutely continuous, which implies bounded variation. For the total variation, use the fact that between consecutive zeros of the derivative, the function is monotone, so the total variation equals the sum of absolute changes over these monotone intervals.

\bigskip\noindent\textbf{Solution:}
Polynomials are continuously differentiable (meaning they have continuous derivatives), hence absolutely continuous on $[a,b]$, so $f\in BV[a,b]$.\footnote{Here $BV[a,b]$ denotes the set of all functions of bounded variation on the interval $[a,b]$.} 

To find the total variation without integration, we can use the fact that between any two consecutive zeros of $f'$, the function $f$ is monotone (since $f'$ doesn't change sign). If $a=t_0<t_1<\cdots<t_m<t_{m+1}=b$ are the ordered zeros of $f'$ in $(a,b)$, then $f$ is monotone on each $[t_j,t_{j+1}]$ and the total variation is simply the sum of the absolute changes over these intervals:
\[V_f(a,b)=\sum_{j=0}^{m}\big|f(t_{j+1})-f(t_j)\big|.\]\qed


\begin{problembox}[6.4: Linear Space of Functions]
\begin{problemstatement}
A nonempty set $S$ of real-valued functions defined on an interval $[a, b]$ is called a linear space of functions if it has the following two properties:

\begin{enumerate}[label=(\alph*)]
\item If $f \in S$, then $cf \in S$ for every real number $c$.

\item If $f \in S$ and $g \in S$, then $f + g \in S$.
\end{enumerate}

Theorem 6.9 shows that the set $V$ of all functions of bounded variation on $[a, b]$ is a linear space. If $S$ is any linear space which contains all monotonic functions on $[a, b]$, prove that $V \subseteq S$. This can be described by saying that the functions of bounded variation form the smallest linear space containing all monotonic functions.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use Jordan's theorem which states that every function of bounded variation can be written as the difference of two increasing functions. Since $S$ contains all monotonic functions and is closed under linear combinations, it must contain all functions of bounded variation.

\bigskip\noindent\textbf{Solution:}
We need to show that if $S$ is any linear space containing all monotonic functions on $[a,b]$, then $S$ must also contain all functions of bounded variation on $[a,b]$.

The key insight is Jordan's decomposition theorem, which tells us that \fbox{every function of bounded variation can be written as the difference of two increasing functions}. 

Let $f$ be any function of bounded variation on $[a,b]$. By Jordan's theorem (Theorem 6.13), there exist increasing functions $g$ and $h$ on $[a,b]$ such that:
\[f(x) = g(x) - h(x) \quad \text{for all } x \in [a,b].\]

Now, since $S$ contains all monotonic functions on $[a,b]$ by hypothesis, we have:
\begin{enumerate}[label=\alph*)]
\item $g \in S$ (because $g$ is increasing, hence monotonic)
\item $h \in S$ (because $h$ is increasing, hence monotonic)
\end{enumerate}

Since $S$ is a linear space, it has the following properties:
\begin{enumerate}[label=\alph*)]
\item If $h \in S$, then $-h \in S$ (by taking $c = -1$ in property (a))
\item If $g \in S$ and $-h \in S$, then $g + (-h) = g - h \in S$ (by property (b))
\end{enumerate}

Therefore, $f = g - h \in S$.

Since this argument works for any function $f$ of bounded variation on $[a,b]$, we conclude that $V \subseteq S$.

\textbf{What this means:} This result shows that the set of functions of bounded variation is the "smallest" linear space that contains all monotonic functions. Any other linear space that contains all monotonic functions must be at least as large as the set of bounded variation functions. This gives us a fundamental characterization of bounded variation functions in terms of monotonic functions.\qed



\begin{problembox}[6.5: Monotonic Function Properties]
\begin{problemstatement}
Let $f$ be a real-valued function defined on $[0, 1]$ such that $f(0) > 0$, $f(x) \neq x$ for all $x$, and $f(x) \leq f(y)$ whenever $x \leq y$. Let $A = \{x: f(x) > x\}$. Prove that $\sup A \in A$ and that $f(1) > 1$.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use proof by contradiction. Assume there exists a set $B = \{x: f(x) < x\}$ and show this leads to a point where $f(x) = x$, contradicting the hypothesis. Use the monotonicity of $f$ and the properties of suprema and infima to establish the result.

\bigskip\noindent\textbf{Solution:}
We are given a function $f$ on $[0,1]$ with the following properties:
\begin{enumerate}[label=\alph*)]
\item $f(0) > 0$ (the function starts above the line $y = x$)
\item $f(x) \neq x$ for all $x \in [0,1]$ (the function never touches the line $y = x$)
\item $f(x) \leq f(y)$ whenever $x \leq y$ (the function is increasing)
\end{enumerate}

We need to prove two things:
\begin{enumerate}[label=\alph*)]
\item $\sup A \in A$ (the supremum of the set where $f(x) > x$ is actually in that set)
\item $f(1) > 1$ (the function ends above the line $y = x$)
\end{enumerate}

Let $A = \{x \in [0,1] : f(x) > x\}$ be the set of points where the function is above the line $y = x$.

\textbf{Step 1:} Since $f(0) > 0$, we have $0 \in A$, so $A$ is non-empty.

\textbf{Step 2:} We will use proof by contradiction. Suppose that $B = \{x \in [0,1] : f(x) < x\}$ is also non-empty. This would mean there are some points where $f(x) > x$ and other points where $f(x) < x$.

Let $s = \sup A$ and $t = \inf B$. Since $A$ and $B$ are both non-empty and $A$ contains points to the left of points in $B$ (because $f$ is increasing), we have $s \leq t$.

\textbf{Step 3:} Now we'll show this leads to a contradiction.

Since $s = \sup A$, there exists a sequence $(x_n)$ in $A$ such that $x_n \uparrow s$. For each $n$, we have $f(x_n) > x_n$. By the monotonicity of $f$, we get:
\[f(s) \geq \limsup_{n \to \infty} f(x_n) \geq \limsup_{n \to \infty} x_n = s.\]
Therefore, $f(s) \geq s$.

Similarly, since $t = \inf B$, there exists a sequence $(y_n)$ in $B$ such that $y_n \downarrow t$. For each $n$, we have $f(y_n) < y_n$. By the monotonicity of $f$, we get:
\[f(t) \leq \liminf_{n \to \infty} f(y_n) \leq \liminf_{n \to \infty} y_n = t.\]
Therefore, $f(t) \leq t$.

\textbf{Step 4:} The contradiction.

Since $s \leq t$ and $f$ is increasing, we have $f(s) \leq f(t)$. But we also have $f(s) \geq s$ and $f(t) \leq t$. This means:
\[s \leq f(s) \leq f(t) \leq t.\]

If $s = t$, then we have $s \leq f(s) \leq s$, which means $f(s) = s$. But this contradicts our assumption that $f(x) \neq x$ for all $x$.

If $s < t$, then there exists some point $u$ with $s < u < t$. Since $u > s = \sup A$, we have $u \notin A$, so $f(u) \leq u$. Since $u < t = \inf B$, we have $u \notin B$, so $f(u) \geq u$. Therefore, $f(u) = u$, which again contradicts our assumption.

\textbf{Step 5:} Conclusion.

Since our assumption that $B$ is non-empty leads to a contradiction, we must have $B = \emptyset$. This means that $f(x) \geq x$ for all $x \in [0,1]$. But since $f(x) \neq x$ for all $x$, we actually have $f(x) > x$ for all $x \in [0,1]$.

Therefore:
\begin{enumerate}[label=\alph*)]
\item $A = [0,1]$, so $\sup A = 1 \in A$
\item $f(1) > 1$
\end{enumerate}

\textbf{What this means:} This result shows that if a monotonic function starts above the line $y = x$ and never touches it, then it must stay above the line for the entire interval. The function cannot "cross over" to below the line without touching it, which would violate the monotonicity condition.\qed



\begin{problembox}[6.6: Bounded Variation on Infinite Intervals]
\begin{problemstatement}
If $f$ is defined everywhere in $\mathbb{R}^1$, then $f$ is said to be of bounded variation on $(-\infty, +\infty)$ if $f$ is of bounded variation on every finite interval and if there exists a positive number $M$ such that $V_f(a, b) < M$ for all compact intervals $[a, b]$. The total variation of $f$ on $(-\infty, +\infty)$ is then defined to be the sup of all numbers $V_f(a, b)$, $-\infty < a < b < +\infty$, and is denoted by $V_f(-\infty, +\infty)$. Similar definitions apply to half-open infinite intervals $[a, +\infty)$ and $(-\infty, b]$.

\begin{enumerate}[label=(\alph*)]
\item State and prove theorems for the infinite interval $(-\infty, +\infty)$ analogous to Theorems 6.7, 6.9, 6.10, 6.11, and 6.12.

\item Show that Theorem 6.5 is true for $(-\infty, +\infty)$ if "monotonic" is replaced by "bounded and monotonic." State and prove a similar modification of Theorem 6.13.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For (a), extend the finite interval results by taking suprema over all finite intervals. For (b), use the fact that on infinite intervals, monotonic functions must be bounded to have finite total variation, and modify Jordan's theorem accordingly.

\bigskip\noindent\textbf{Solution:}
\,(a) With $V_f(-\infty,\infty)=\sup\{V_f(a,b): a<b\}$, all finite-interval results extend: linearity and subadditivity of variation, stability under addition and scalar multiplication, and the characterization $f\in BV(-\infty,\infty)$ iff $V_f(-\infty,\infty)<\infty$. Proofs reduce to restricting to finite $[a,b]$ and taking sups.

\,(b) For $(-\infty,\infty)$, a monotone function has
\[V_f(-\infty,\infty)=\lim_{x\to\infty}f(x)-\lim_{x\to-\infty}f(x),\]
so it is of bounded variation iff it is bounded and monotone. Similarly, Jordanâ€™s theorem becomes: $f\in BV(-\infty,\infty)$ iff $f=g-h$ with $g,h$ bounded monotone on $\mathbb{R}$.\qed


\begin{problembox}[6.7: Positive and Negative Variations]
\begin{problemstatement}
Assume that $f$ is of bounded variation on $[a, b]$ and let
\[P = \{x_0, x_1, \ldots, x_n\} \in \mathcal{P}[a, b].\]
As usual, write $\Delta f_k = f(x_k) - f(x_{k-1})$, $k = 1, 2, \ldots, n$. Define
\[A(P) = \{k : \Delta f_k > 0\}, \quad B(P) = \{k : \Delta f_k < 0\}.\]
The numbers
\[p_f(a, b) = \sup \left\{ \sum_{k \in A(P)} \Delta f_k : P \in \mathcal{P}[a, b] \right\}\]
and
\[n_f(a, b) = \sup \left\{ \sum_{k \in B(P)} |\Delta f_k| : P \in \mathcal{P}[a, b] \right\}\]
are called, respectively, the positive and negative variations of $f$ on $[a, b]$. For each $x$ in $(a, b]$, let $V(x) = V_f(a, x)$, $p(x) = p_f(a, x)$, $n(x) = n_f(a, x)$, and let $V(a) = p(a) = n(a) = 0$. Show that we have:
\begin{enumerate}[label=\alph*)]
\item $V(x) = p(x) + n(x)$.
\item $0 \leq p(x) \leq V(x)$ and $0 \leq n(x) \leq V(x)$.
\item $p$ and $n$ are increasing on $[a, b]$.
\item $f(x) = f(a) + p(x) - n(x)$. Part (d) gives an alternative proof of Theorem 6.13.
\item $2p(x) = V(x) + f(x) - f(a)$, $2n(x) = V(x) - f(x) + f(a)$.
\item Every point of continuity of $f$ is also a point of continuity of $p$ and of $n$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the definition of total variation as a supremum over partitions and separate positive and negative increments. For (d), use the fact that any partition's sum of increments equals the total change in function value. For (e), combine the results from (a) and (d). For (f), use the continuity of the total variation function.

\bigskip\noindent\textbf{Solution:}
Let's understand what we're trying to prove. We have a function $f$ of bounded variation, and we want to break it down into its "upward" and "downward" movements.

For any point $x$ in $[a,b]$, we can look at how $f$ changes from $a$ to $x$. The total variation $V(x)$ measures how much $f$ "wiggles" up and down on the interval $[a,x]$. We want to separate this into positive variation $p(x)$ (how much it goes up) and negative variation $n(x)$ (how much it goes down).

(a) and (b): When we look at any partition of $[a,x]$, we can separate the changes into positive ones (where $f$ increases) and negative ones (where $f$ decreases). The total variation is just the sum of all these changes in absolute value, which equals the sum of positive changes plus the sum of negative changes. This gives us $V(x) = p(x) + n(x)$. Since both $p(x)$ and $n(x)$ are sums of positive numbers, they must be between $0$ and $V(x)$.

(c): As we move $x$ further to the right, we're considering longer intervals, so both the positive and negative variations can only increase. This means $p$ and $n$ are increasing functions.

(d): This is the key insight! For any partition of $[a,x]$, the total change in $f$ from $a$ to $x$ is just the sum of all the small changes: $f(x) - f(a) = \sum \Delta f_k$. But this sum equals the sum of positive changes minus the sum of negative changes. When we take the supremum over all partitions, we get $f(x) = f(a) + p(x) - n(x)$. This shows that any function of bounded variation can be written as the difference of two increasing functions.

(e): We can solve for $p(x)$ and $n(x)$ using the equations from (a) and (d). Adding them gives $2p(x) = V(x) + f(x) - f(a)$, and subtracting gives $2n(x) = V(x) - f(x) + f(a)$.

(f): If $f$ is continuous at a point, then the total variation $V$ is also continuous there. Since $p$ and $n$ are expressed in terms of $V$ and $f$ (from part (e)), they must also be continuous at that point.\qed
\section{Curves}

\begin{definitionssection}{Definitions and Theorems}
\end{definitionssection}

\begin{definition}[Rectifiable Curve]
A curve $\gamma$ in $\mathbb{R}^n$ is called rectifiable if it has finite length. The length of a rectifiable curve is defined as the supremum of the lengths of all inscribed polygonal approximations.
\end{definition}

\begin{importance}
\noindent\textbf{Importance:} Rectifiable curves are the geometric counterpart of functions of bounded variation. They provide a rigorous foundation for measuring curve lengths and are essential for understanding geometric properties of curves in higher dimensions. This concept is crucial for differential geometry and calculus on manifolds.
\end{importance}



\begin{definition}[Equivalent Paths]
Two paths $f$ and $g$ are equivalent if there exists a strictly increasing continuous function $\phi$ mapping the domain of $g$ onto the domain of $f$ such that $g = f \circ \phi$.
\end{definition}

\noindent\begin{importance}
\textbf{Importance:}Path equivalence provides a way to identify paths that trace the same geometric curve but with different parametrizations. This is essential for understanding the geometric properties of curves independently of their specific parametrization and is fundamental for differential geometry.
\end{importance}\begin{theorem}[Arc-Length Parameterization]
Every rectifiable curve can be reparametrized by arc length, giving a parametrization where the parameter represents the distance traveled along the curve.
\end{theorem}

\noindent\begin{importance}
\textbf{Importance:}Arc-length parameterization provides a natural and geometrically meaningful way to parametrize curves. It simplifies many calculations and is essential for understanding the intrinsic geometry of curves, independent of any particular parametrization.
\end{importance}\begin{theorem}[Length of Curve]
If $f$ is a continuously differentiable function on $[a, b]$, then the length of the curve described by $f$ can be computed as the limit of the lengths of inscribed polygonal approximations. For practical purposes, this is often expressed as:
\[\Lambda_f(a, b) = \int_a^b \|f'(t)\| \, dt\]
\end{theorem}

\noindent\begin{importance}
\textbf{Importance:}This result connects the geometric concept of length with the analytical concept of the derivative. While the integral formula is useful for computation, the fundamental idea is that the length is the supremum of all polygonal approximations, which provides a rigorous foundation for measuring curve lengths.
\end{importance}\begin{theorem}[Symmetrization of Curves]
The symmetrization of a curve preserves its length while creating a symmetric version with respect to a given axis.
\end{theorem}

\noindent\begin{importance}
\textbf{Importance:}Symmetrization is a powerful geometric technique that preserves important properties while simplifying the geometry. It's useful for proving geometric inequalities and for understanding the relationship between curves and their symmetric counterparts.
\end{importance}
\begin{problembox}[6.8: Equivalent Paths]
\begin{problemstatement}
Let $f$ and $g$ be complex-valued functions defined as follows:
\[f(t) = e^{2\pi it} \quad \text{if } t \in [0, 1], \quad g(t) = e^{2\pi it} \quad \text{if } t \in [0, 2].\]

\begin{enumerate}[label=(\alph*)]
\item Prove that $f$ and $g$ have the same graph but are not equivalent according to the definition in Section 6.12.

\item Prove that the length of $g$ is twice that of $f$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For (a), show that the graphs are identical as sets but the functions traverse the circle different numbers of times, making them inequivalent. For (b), use the fact that repeating a path doubles its length.

\bigskip\noindent\textbf{Solution:}
As sets, $\{e^{2\pi it}:t\in[0,1]\}=\{e^{2\pi it}:t\in[0,2]\}$, so the graphs coincide (the unit circle). If $f$ and $g$ were equivalent, there would be a strictly increasing bijection $\phi:[0,2]\to[0,1]$ with $g=f\circ\phi$, impossible since $g$ traverses the circle twice while $f$ traverses it once. For lengths, repeating a rectifiable path twice doubles its length, so $\Lambda(g)=2\Lambda(f)$.\qed


\begin{problembox}[6.9: Arc-Length Parameter]
\begin{problemstatement}
Let $f$ be a rectifiable path of length $L$ defined on $[a, b]$, and assume that $f$ is not constant on any subinterval of $[a, b]$. Let $s$ denote the arc-length function given by $s(x) = \Lambda_s(a, x)$ if $a < x \leq b$, $s(a) = 0$.

\begin{enumerate}[label=(\alph*)]
\item Prove that $s^{-1}$ exists and is continuous on $[0, L]$.
\item Define $g(t) = f[s^{-1}(t)]$ if $t \in [0, L]$ and show that $g$ is equivalent to $f$. Since $f(t) = g[s(t)]$, the function $g$ is said to provide a representation of the graph of $f$ with arc length as parameter.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For (a), use the fact that the arc-length function is strictly increasing and continuous, making it a homeomorphism. For (b), show that the composition with the inverse arc-length function provides an equivalent parametrization.

\bigskip\noindent\textbf{Solution:}
\,(a) The arc-length function $s$ is increasing and continuous on $[a,b]$, and strictly increasing under the hypothesis that $f$ is not constant on any subinterval. Hence $s$ is a homeomorphism from $[a,b]$ onto $[0,L]$ and $s^{-1}$ is continuous.

\,(b) With $g(t)=f\big(s^{-1}(t)\big)$, the map $t\mapsto s^{-1}(t)$ is increasing and onto, so $g$ is a reparametrization of $f$; thus $g$ is equivalent to $f$ and $f(t)=g\big(s(t)\big)$.\qed


\begin{problembox}[6.10: Symmetrization of Regions]
\begin{problemstatement}
Let $f$ and $g$ be two real-valued continuous functions of bounded variation defined on $[a, b]$, with $0 < f(x) < g(x)$ for each $x$ in $(a, b)$, $f(a) = g(a)$, $f(b) = g(b)$. Let $h$ be the complex-valued function defined on the interval $[a, 2b - a]$ as follows:
\[h(t) = t + i f(t), \quad \text{if } a \leq t \leq b,\]
\[h(t) = 2b - t + ig(2b - t), \quad \text{if } b \leq t \leq 2b - a.\]

\begin{enumerate}[label=(\alph*)]
\item Show that $h$ describes a rectifiable curve $\Gamma$.
\item Explain, by means of a sketch, the geometric relationship between $f$, $g$, and $h$.
\item Show that the set of points
\[S = \{ (x, y) : a \leq x \leq b, \quad f(x) \leq y \leq g(x) \}\]
is a region in $\mathbb{R}^2$ whose boundary is the curve $\Gamma$.
\item Let $H$ be the complex-valued function defined on $[a, 2b - a]$ as follows:
\[H(t) = t - \frac{1}{2} [g(t) - f(t)], \quad \text{if } a \leq t \leq b,\]
\[H(t) = t + \frac{1}{2} [g(2b - t) - f(2b - t)], \quad \text{if } b \leq t \leq 2b - a.\]
Show that $H$ describes a rectifiable curve $\Gamma_0$ which is the boundary of the region
\[S_0 = \{ (x, y) : a \leq x \leq b, \quad f(x) - g(x) \leq 2y \leq g(x) - f(x) \}.\]
\item Show that $S_0$ has the $x$-axis as a line of symmetry. (The region $S_0$ is called the symmetrization of $S$ with respect to the $x$-axis.)
\item Show that the length of $\Gamma_0$ does not exceed the length of $\Gamma$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For (a), use the fact that functions of bounded variation have rectifiable graphs. For (b)-(c), understand that $h$ traces the boundary of the region between the two graphs. For (d)-(e), show that $H$ creates a symmetric region by averaging the upper and lower boundaries. For (f), use geometric reasoning and the triangle inequality for arc lengths rather than integration.

\bigskip\noindent\textbf{Solution:}
\,(a) Since $f,g\in BV[a,b]$, the graphs $t\mapsto t+if(t)$ and $t\mapsto t+ig(t)$ are rectifiable; concatenating them as in the definition of $h$ yields a rectifiable closed curve $\Gamma$.

\,(b) The curve $\Gamma$ runs along the upper graph $y=g(x)$ from $x=a$ to $x=b$ and returns along the lower graph $y=f(x)$ from $x=b$ to $x=a$, closing the boundary of the vertical strip between the two graphs.

\,(c) The region $S=\{(x,y): a\le x\le b,\ f(x)\le y\le g(x)\}$ has boundary given by the two graphs $y=f(x)$ and $y=g(x)$ together with the vertical segments at $x=a$ and $x=b$, which is exactly the image of $h$.

\,(d) Writing $y_0(x)=\tfrac12\big(g(x)-f(x)\big)$, the curve $\Gamma_0$ is traced by $x\mapsto x\pm i y_0(x)$, hence is rectifiable and bounds the symmetric vertical strip
\[S_0=\{(x,y): a\le x\le b,\ -y_0(x)\le y\le y_0(x)\}.\]

\,(e) Immediate from the definition of $S_0$ since $y\mapsto -y$ preserves the set.

\,(f) To show that the length of $\Gamma_0$ does not exceed the length of $\Gamma$, we can use a geometric argument. The curve $\Gamma$ consists of two parts: one following the upper graph $y=g(x)$ and one following the lower graph $y=f(x)$. The curve $\Gamma_0$ is symmetric about the $x$-axis and essentially averages these two graphs.

By the triangle inequality for arc lengths, the length of $\Gamma_0$ must be less than or equal to the sum of the lengths of the two graphs that make up $\Gamma$. This is because $\Gamma_0$ represents a "straightened out" version of the boundary, and straightening can only decrease or maintain the total length. Therefore, $L(\Gamma) \geq L(\Gamma_0)$.\qed
\section{Absolute continuous functions}

\begin{definitionssection}{Definitions and Theorems}
\end{definitionssection}

\begin{definition}[Absolute Continuity]
A real-valued function $f$ defined on an interval $[a,b]$ is absolutely continuous on $[a,b]$ if for every $\varepsilon > 0$ there is a $\delta > 0$ such that
\[\sum_{k=1}^{n} |f(b_k) - f(a_k)| < \varepsilon\]
for every $n$ disjoint open subintervals $(a_k, b_k)$ of $[a, b]$, $n = 1, 2, \ldots$, the sum of whose lengths $\sum_{k=1}^{n} (b_k - a_k)$ is less than $\delta$.
\end{definition}

\noindent\begin{importance}
\textbf{Importance:}Absolute continuity is a stronger form of continuity that is essential for the Lebesgue theory of integration and differentiation.
\end{importance}\begin{definition}[Lipschitz Condition]
A function $f$ satisfies a Lipschitz condition of order $\alpha > 0$ on $[a, b]$ if there exists a constant $M > 0$ such that $|f(x) - f(y)| \leq M |x - y|^\alpha$ for all $x, y \in [a, b]$.
\end{definition}

\noindent\begin{importance}
\textbf{Importance:}Lipschitz conditions provide a quantitative measure of how much a function can change and are essential for understanding function regularity.
\end{importance}\begin{theorem}[Absolute Continuity Implies Bounded Variation]
Every absolutely continuous function on $[a, b]$ is continuous and of bounded variation on $[a, b]$.
\end{theorem}

\noindent\begin{importance}
\textbf{Importance:}This theorem shows that absolute continuity is a stronger condition than both continuity and bounded variation. It establishes the hierarchy of function classes and is essential for understanding the relationship between different notions of regularity.
\end{importance}\begin{theorem}[Lipschitz Implies Absolute Continuity]
If $f$ satisfies a uniform Lipschitz condition of order 1 on $[a, b]$, then $f$ is absolutely continuous.
\end{theorem}

\noindent\begin{importance}
\textbf{Importance:}This theorem provides a practical way to verify absolute continuity using Lipschitz conditions. It's particularly useful for functions that arise in applications, where Lipschitz conditions are often easier to check than the abstract definition of absolute continuity.
\end{importance}\begin{theorem}[Operations on Absolutely Continuous Functions]
If $f$ and $g$ are absolutely continuous on $[a, b]$, then so are $|f|$, $cf$ (for any constant $c$), $f + g$, $f \cdot g$, and $f/g$ (if $g$ is bounded away from zero).
\end{theorem}

\noindent\begin{importance}
\textbf{Importance:}This theorem shows that the class of absolutely continuous functions is closed under common operations. This makes it a robust class for analysis and ensures that many naturally occurring functions preserve this important property.
\end{importance}
\begin{problembox}[6.11: Absolutely Continuous Functions]
\begin{problemstatement}
A real-valued function $f$ defined on $[a, b]$ is said to be absolutely continuous on $[a, b]$ if for every $\varepsilon > 0$ there is a $\delta > 0$ such that
\[\sum_{k=1}^{n} |f(b_k) - f(a_k)| < \varepsilon\]
for every $n$ disjoint open subintervals $(a_k, b_k)$ of $[a, b]$, $n = 1, 2, \ldots$, the sum of whose lengths $\sum_{k=1}^{n} (b_k - a_k)$ is less than $\delta$. Absolutely continuous functions occur in the Lebesgue theory of integration and differentiation. The following exercises give some of their elementary properties.

Prove that every absolutely continuous function on $[a, b]$ is continuous and of bounded variation on $[a, b]$. Note. There exist functions which are continuous and of bounded variation but not absolutely continuous.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the definition of absolute continuity to show uniform continuity, which implies continuity. For bounded variation, use the same $\delta$ from the definition to bound the total variation by choosing a partition with mesh less than $\delta$.

\bigskip\noindent\textbf{Solution:}
Absolute continuity implies uniform continuity; hence $f$ is continuous. Given $\varepsilon>0$, choose $\delta$ from the definition. For any partition $P$ with mesh $<\delta$,
\[\sum_k |f(x_k)-f(x_{k-1})|\le\varepsilon,\]
so $V_f(a,b)<\infty$. Thus every absolutely continuous function is continuous and of bounded variation.\qed


\begin{problembox}[6.12: Lipschitz and Absolute Continuity]
\begin{problemstatement}
Prove that $f$ is absolutely continuous if it satisfies a uniform Lipschitz condition of order 1 on $[a, b]$. (See Exercise 6.2.)
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the Lipschitz condition to bound the sum of function differences in terms of the sum of interval lengths, then choose $\delta$ appropriately to satisfy the absolute continuity definition.

\bigskip\noindent\textbf{Solution:}
If $|f(x)-f(y)|\le M|x-y|$ on $[a,b]$, then for any disjoint intervals $(a_k,b_k)$ with $\sum(b_k-a_k)<\delta$, we have
\[\sum_{k=1}^n |f(b_k)-f(a_k)|\le M\sum_{k=1}^n(b_k-a_k)<M\delta.
\]
Choosing $\delta=\varepsilon/M$ proves absolute continuity.\qed


\begin{problembox}[6.13: Operations on Absolutely Continuous Functions]
\begin{problemstatement}
If $f$ and $g$ are absolutely continuous on $[a, b]$, prove that each of the following is also: $|f|$, $cf$ ($c$ constant), $f + g$, $f \cdot g$; also $f/g$ if $g$ is bounded away from zero.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the triangle inequality for $|f|$, linearity for $cf$ and $f+g$, the product rule and boundedness for $f \cdot g$, and the fact that $1/g$ is Lipschitz when $g$ is bounded away from zero for $f/g$.

\bigskip\noindent\textbf{Solution:}
If $f,g$ are absolutely continuous on $[a,b]$, then they are bounded. The functions $cf$ and $f+g$ are absolutely continuous by linearity of the defining inequality. Also
\[\big||f(b_k)|-|f(a_k)|\big|\le |f(b_k)-f(a_k)|,\]
so $|f|$ is absolutely continuous. For the product,
\[|(fg)(b_k)-(fg)(a_k)|\le |f(b_k)|\,|g(b_k)-g(a_k)|+|g(a_k)|\,|f(b_k)-f(a_k)|,\]
and summing over $k$ shows $fg$ is absolutely continuous. If $|g|\ge m>0$ on $[a,b]$, then $u\mapsto 1/u$ is Lipschitz on $[m,\infty)$, hence $1/g$ is absolutely continuous; therefore $f/g=f\cdot(1/g)$ is absolutely continuous.\qed

\begin{techniquessection}[Solving and Proving Techniques]

\subsection*{Proving Bounded Variation}
\begin{itemize}
\item Show the derivative is bounded to establish Lipschitz continuity, which implies absolute continuity and hence bounded variation
\item Construct specific partitions using points where the function oscillates to show total variation is infinite
\item Use Jordan's theorem: every function of bounded variation can be written as the difference of two increasing functions
\item For polynomials, use the fact that they are continuously differentiable (have continuous derivatives), hence absolutely continuous
\item For monotone functions, use the fact that total variation equals the absolute change in function values
\end{itemize}

\subsection*{Working with Lipschitz Conditions}
\begin{itemize}
\item Use subdivision arguments to show that $\alpha > 1$ implies the function is constant
\item For $\alpha = 1$, show the function is Lipschitz and hence absolutely continuous
\item Construct Weierstrass-type functions with appropriate scaling for examples
\item Use step functions which have bounded variation but are discontinuous
\end{itemize}

\subsection*{Proving Absolute Continuity}
\begin{itemize}
\item Use the definition to show uniform continuity, which implies continuity
\item For bounded variation, use the same $\delta$ from the definition to bound total variation
\item Apply Lipschitz conditions to bound function differences in terms of interval lengths
\item Use triangle inequality and linearity properties for operations on absolutely continuous functions
\end{itemize}

\subsection*{Analyzing Curves and Paths}
\begin{itemize}
\item Show that graphs coincide as sets but functions traverse different numbers of times
\item Use the fact that repeating a path doubles its length
\item Apply the arc-length function's strictly increasing and continuous properties
\item Use the fact that functions of bounded variation have rectifiable graphs
\end{itemize}

\subsection*{Geometric Constructions}
\begin{itemize}
\item Trace boundaries of regions between two graphs
\item Create symmetric regions by averaging upper and lower boundaries
\item Use triangle inequality for arc lengths and geometric reasoning
\item Apply symmetrization techniques to preserve geometric properties
\item When possible, use geometric arguments instead of integration to compare lengths
\end{itemize}
\end{techniquessection}