\chapter{Multivariable Differential Calculus}

\section{Differentiable Functions}

\subsection*{Essential Definitions and Theorems}

\begin{definition}[Partial Derivatives]
For a function $f: S \to \mathbb{R}$ where $S \subseteq \mathbb{R}^n$, the $k$-th partial derivative at a point $x$ is defined as:
\[D_k f(x) = \lim_{h \to 0} \frac{f(x + he_k) - f(x)}{h}\]
where $e_k$ is the $k$-th standard basis vector.
\end{definition}

\noindent\textbf{Importance:} Partial derivatives measure how a function changes when we vary one variable while keeping the others fixed. They are the building blocks for understanding how multivariable functions behave and are essential for optimization, differential equations, and many applications in physics and engineering.



\begin{definition}[Directional Derivative]
The directional derivative of $f$ at $x$ in the direction of a unit vector $u$ is:
\[f'(x; u) = \lim_{h \to 0} \frac{f(x + hu) - f(x)}{h}\]
\end{definition}

\noindent\textbf{Importance:} Directional derivatives generalize partial derivatives to arbitrary directions. They tell us how fast a function changes as we move in a specific direction, which is crucial for understanding the local behavior of functions and for optimization algorithms.



\begin{definition}[Fréchet Differentiability]
A function $f: S \to \mathbb{R}^m$ is differentiable at an interior point $c$ of $S$ if there exists a linear transformation $Df(c): \mathbb{R}^n \to \mathbb{R}^m$ such that:
\[f(c + h) = f(c) + Df(c)h + o(\|h\|)\]
The matrix representation of $Df(c)$ is called the Jacobian matrix.
\end{definition}

\noindent\textbf{Importance:} Fréchet differentiability provides the most general and powerful notion of differentiability for multivariable functions. It ensures that the function can be well-approximated by a linear function near the point, which is essential for calculus, optimization, and differential equations.



\begin{theorem}[Relationship Between Derivatives]
If $f$ is differentiable at $x$, then:
\begin{enumerate}[label=(\alph*)]
\item All partial derivatives exist at $x$
\item All directional derivatives exist at $x$ and $f'(x; u) = Df(x)u$
\item The Jacobian matrix has entries $[Df(x)]_{ij} = D_j f_i(x)$
\end{enumerate}
\end{theorem}

\noindent\textbf{Importance:} This theorem connects the different notions of derivatives and shows that differentiability is a strong condition that implies the existence of all partial and directional derivatives. The relationship $f'(x; u) = Df(x)u$ is particularly important for applications.



\begin{theorem}[Chain Rule for Multivariable Functions]
If $f: \mathbb{R}^n \to \mathbb{R}^m$ is differentiable at $x$ and $g: \mathbb{R}^m \to \mathbb{R}^p$ is differentiable at $f(x)$, then the composition $g \circ f$ is differentiable at $x$ and:
\[D(g \circ f)(x) = Dg(f(x)) \cdot Df(x)\]
\end{theorem}

\noindent\textbf{Importance:} The chain rule is one of the most fundamental results in calculus. It allows us to compute derivatives of complicated functions by breaking them down into simpler parts. This is essential for optimization, differential equations, and many applications in science and engineering.





\begin{problembox}[12.1: Local Extrema and Partial Derivatives]
\begin{problemstatement}
Let \( S \) be an open subset of \( \mathbb{R}^n \), and let \( f: S \to \mathbb{R} \) be a real-valued function with finite partial derivatives \( D_1f, \ldots, D_nf \) on \( S \). If \( f \) has a local maximum or a local minimum at a point \( c \) in \( S \), prove that \( D_kf(c) = 0 \) for each \( k \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the definition of partial derivatives by restricting to coordinate directions. For each coordinate direction, the function becomes a one-variable function that has a local extremum at the origin, so its derivative must be zero.

\bigskip\noindent\textbf{Solution:}
Fix $k$ and set $\phi(t)=f(c+te_k)$ for small $t$. If $c$ is a local extremum, then $t=0$ is a local extremum of the one-variable function $\phi$, hence $\phi'(0)=0$. But $\phi'(0)=D_k f(c)$ by definition, so $D_k f(c)=0$ for each $k$.\qed


\begin{problembox}[12.2: Partial and Directional Derivatives]
\begin{problemstatement}
Calculate all first-order partial derivatives and the directional derivative \( f'(x; u) \) for each of the real-valued functions defined on \( \mathbb{R}^n \) as follows:
\begin{enumerate}[label=(\alph*)]
\item \( f(x) = a \cdot x \), where \( a \) is a fixed vector in \( \mathbb{R}^n \).
\item \( f(x) = \|x\|^4 \).
\item \( f(x) = x \cdot L(x) \), where \( L : \mathbb{R}^n \to \mathbb{R}^n \) is a linear function.
\item \( f(x) = \sum_{i=1}^{n} \sum_{j=1}^{n} a_{ij}x_i x_j \), where \( a_{ij} = a_{ji} \).
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Compute partial derivatives using standard differentiation rules, then use the relation \( f'(x; u) = \nabla f(x) \cdot u \) to find directional derivatives. For linear functions, use matrix notation; for quadratic forms, use the symmetry property.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item $D_i f(x)=a_i$, so $\nabla f=a$. Hence $f'(x;u)=a\cdot u$.
\item Write $r^2=\|x\|^2$. Then $D_i f(x)=4r^2 x_i$ and $\nabla f(x)=4r^2 x$. Thus $f'(x;u)=\nabla f\cdot u=4\|x\|^2(x\cdot u)$.
\item Let $L(x)=Ax$ for some matrix $A$. Then $f(x)=x\cdot Ax$ and $\nabla f(x)=(A+A^{\!\top})x$. Thus $D_i f(x)=[(A+A^{\!\top})x]_i$ and $f'(x;u)=u\cdot (A+A^{\!\top})x$.
\item With $A=(a_{ij})$ symmetric, $f(x)=x^{\!\top}Ax$, so $\nabla f(x)=2Ax$, $D_i f(x)=[2Ax]_i$, and $f'(x;u)=2u\cdot Ax$.
\end{enumerate}\qed


\begin{problembox}[12.3: Directional Derivatives of Sum and Product]
\begin{problemstatement}
Let \( f \) and \( g \) be functions with values in \( \mathbb{R}^m \) such that the directional derivatives \( f'(c; u) \) and \( g'(c; u) \) exist. Prove that the sum \( f + g \) and dot product \( f \cdot g \) have directional derivatives given by
\[(f + g)'(c; u) = f'(c; u) + g'(c; u)\]
and
\[(f \cdot g)'(c; u) = f(c) \cdot g'(c; u) + g(c) \cdot f'(c; u).\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the definition of directional derivatives as limits of difference quotients. For the sum, apply linearity of limits. For the product, expand the difference quotient and use the product rule for dot products.

\bigskip\noindent\textbf{Solution:}
For $f+g$, divide the increment $[f(c+tu)+g(c+tu)]-[f(c)+g(c)]$ by $t$ and pass to the limit. For $f\cdot g$, expand
\begin{align*}
&\frac{f(c+tu)\cdot g(c+tu)-f(c)\cdot g(c)}{t} \\
=&f(c)\cdot\frac{g(c+tu)-g(c)}{t}+g(c)\cdot\frac{f(c+tu)-f(c)}{t}+o(1),
\end{align*}
and take $t\to0$.\qed


\begin{problembox}[12.4: Differentiability of Vector-Valued Functions]
\begin{problemstatement}
If \( S \subseteq \mathbb{R}^n \), let \( f: S \to \mathbb{R}^m \) be a function with values in \( \mathbb{R}^m \), and write \( f = (f_1, \ldots, f_m) \). Prove that \( f \) is differentiable at an interior point \( c \) of \( S \) if, and only if, each \( f_i \) is differentiable at \( c \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the component-wise definition of differentiability. If \( f \) is differentiable, project to each component to show each \( f_i \) is differentiable. Conversely, if each \( f_i \) is differentiable, construct the Jacobian matrix and show the remainder term is \( o(\|h\|) \).

\bigskip\noindent\textbf{Solution:}
If $f$ is differentiable, then $f(c+h)=f(c)+Df(c)h+o(\|h\|)$; projecting to the $i$th coordinate gives the differentiability of $f_i$ with derivative the $i$th row of $Df(c)$. Conversely, if each $f_i$ is differentiable, stack their linear maps to form the Jacobian $Df(c)$ and note $\|f(c+h)-f(c)-Df(c)h\|\le\sum_i |f_i(c+h)-f_i(c)-Df_i(c)h|=o(\|h\|)$.\qed


\begin{problembox}[12.5: Differentiability of Sum of Univariate Functions]
\begin{problemstatement}
Given \( n \) real-valued functions \( f_1, \ldots, f_n \), each differentiable on an open interval \( (a, b) \) in \( \mathbb{R} \). For each \( x = (x_1, \ldots, x_n) \) in the \( n \)-dimensional open interval
\[S = \{(x_1, \ldots, x_n): a < x_k < b, \quad k = 1, 2, \ldots, n\},\]
define \( f(x) = f_1(x_1) + \cdots + f_n(x_n) \). Prove that \( f \) is differentiable at each point of \( S \) and that
\[f'(x)(u) = \sum_{i=1}^{n} f_i'(x_i)u_i, \quad \text{where } u = (u_1, \ldots, u_n).\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Since each term depends only on one coordinate, the Jacobian matrix is diagonal. Use the fact that each \( f_i \) is differentiable in one variable to show the remainder term is \( o(\|u\|) \).

\bigskip\noindent\textbf{Solution:}
Each term depends on one coordinate, so $Df(x)$ is diagonal with entries $f_i'(x_i)$. Hence $f'(x)(u)=\sum_i f_i'(x_i)u_i$ and the remainder is $o(\|u\|)$ by the 1D differentiability of each $f_i$.\qed


\begin{problembox}[12.6: Differentiability with Partial Limits]
\begin{problemstatement}
Given \( n \) real-valued functions \( f_1, \ldots, f_n \) defined on an open set \( S \) in \( \mathbb{R}^n \). For each \( x \) in \( S \), define \( f(x) = f_1(x) + \cdots + f_n(x) \). Assume that for each \( k = 1, 2, \ldots, n \), the following limit exists:
\[\lim_{\substack{y \to x \\ y_k \neq x_k}} \frac{f_k(y) - f_k(x)}{y_k - x_k}.\]
Call this limit \( a_k(x) \). Prove that \( f \) is differentiable at \( x \) and that
\[f'(x)(u) = \sum_{k=1}^{n} a_k(x) u_k \quad \text{if } u = (u_1, \ldots, u_n).\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Approximate the change in \( f \) by varying coordinates one at a time, using the given partial limits. This creates a telescoping sum that gives the linear approximation with remainder term \( o(\|y-x\|) \).

\bigskip\noindent\textbf{Solution:}
Vary $y$ from $x$ by changing one coordinate at a time: $x=x^{(0)}\to x^{(1)}\to\cdots\to x^{(n)}=y$, where only the $k$th coordinate changes in step $k$. Then
\[f(y)-f(x)=\sum_{k=1}^n\big(f_k(x^{(k)})-f_k(x^{(k-1)})\big)=\sum_{k=1}^n a_k(x)(y_k-x_k)+o(\|y-x\|),\]
by the defining limits for $a_k(x)$. Hence $f$ is differentiable with $Df(x)u=\sum_k a_k(x)u_k$.\qed


\begin{problembox}[12.7: Differentiability of Product at Zero]
\begin{problemstatement}
Let \( f \) and \( g \) be functions from \( \mathbb{R}^n \) to \( \mathbb{R}^m \). Assume that \( f \) is differentiable at \( c \), that \( f(c) = 0 \), and that \( g \) is continuous at \( c \). Let \( h(x) = g(x) \cdot f(x) \). Prove that \( h \) is differentiable at \( c \) and that
\[h'(c)(u) = g(c) \cdot \{f'(c)(u)\} \quad \text{if } u \in \mathbb{R}^n.\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the linear approximation for \( f \) at \( c \) and the continuity of \( g \) to expand \( h(c+h) - h(c) \). The key insight is that \( f(c) = 0 \) simplifies the product rule.

\bigskip\noindent\textbf{Solution:}
Write $f(c+h)=f(c)+f'(c)h+r(h)$ with $\|r(h)\|=o(\|h\|)$ and use continuity of $g$:
\[h(c+h)-h(c)=g(c+h)\cdot f'(c)h+g(c+h)\cdot r(h)=g(c)\cdot f'(c)h+o(\|h\|).\]
Thus $h$ is differentiable with derivative $u\mapsto g(c)\cdot f'(c)u$.\qed


\begin{problembox}[12.8: Jacobian Matrix Calculation]
\begin{problemstatement}
Let \( f : \mathbb{R}^2 \to \mathbb{R}^3 \) be defined by the equation
\[f(x, y) = (\sin x \cos y, \sin x \sin y, \cos x \cos y).\]
Determine the Jacobian matrix \( Df(x, y) \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Compute partial derivatives of each component function with respect to \( x \) and \( y \) using standard differentiation rules for trigonometric functions and the product rule.

\bigskip\noindent\textbf{Solution:}
\[Df(x,y)=\begin{pmatrix}
\cos x\cos y & -\sin x\sin y\\
\cos x\sin y & \sin x\cos y\\
-\sin x\cos y & -\cos x\sin y
\end{pmatrix}.\]\qed


\begin{problembox}[12.9: Nonexistence of Positive Directional Derivative]
\begin{problemstatement}
Prove that there is no real-valued function \( f \) such that \( f'(c; u) > 0 \) for a fixed point \( c \) in \( \mathbb{R}^n \) and every nonzero vector \( u \) in \( \mathbb{R}^n \). Give an example such that \( f'(c; u) > 0 \) for a fixed direction \( u \) and every \( c \) in \( \mathbb{R}^n \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the fact that directional derivatives are linear in the direction vector, so \( f'(c; -u) = -f'(c; u) \). This creates a contradiction if all directional derivatives are positive. For the example, use a linear function.

\bigskip\noindent\textbf{Solution:}
For any $u\ne0$, the 1D definition along lines gives $f'(c;-u)=-f'(c;u)$, so $f'(c;u)>0$ cannot hold for both $u$ and $-u$. For the example with a fixed $u$, take $f(x)=u\cdot x$. Then $f'(c;u)=\|u\|^2>0$ for every $c$.\qed


\begin{problembox}[12.10: Complex Differentiability and Directional Derivatives]
\begin{problemstatement}
Let \( f = u + iv \) be a complex-valued function such that the derivative \( f'(c) \) exists for some complex \( c \). Write \( z = c + re^{i\alpha} \) (where \( \alpha \) is real and fixed) and let \( r \to 0 \) in the difference quotient \( [f(z) - f(c)]/(z - c) \) to obtain
\[f'(c) = e^{-i\alpha}[u'(c; a) + iv'(c; a)],\]
where \( a = (\cos \alpha, \sin \alpha) \), and \( u'(c; a) \) and \( v'(c; a) \) are directional derivatives. Let \( b = (\cos \beta, \sin \beta) \), where \( \beta = \alpha + \frac{1}{2}\pi \), and show by a similar argument that
\[f'(c) = e^{-i\alpha}[v'(c; b) - iu'(c; b)].\]
Deduce that \( u'(c; a) = v'(c; b) \) and \( v'(c; a) = -u'(c; b) \). The Cauchy-Riemann equations (Theorem 5.22) are a special case.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Express the complex derivative in terms of directional derivatives by taking limits along different directions. Use the fact that the complex derivative must be the same regardless of the approach direction, then equate the two expressions to derive the Cauchy-Riemann relations.

\bigskip\noindent\textbf{Solution:}
Along $z=c+re^{i\alpha}$, the complex difference quotient tends to $f'(c)$, while its real and imaginary parts are $u'(c;a)$ and $v'(c;a)$, giving the first identity. Rotating the approach by $\pi/2$ yields the second. Equating the two expressions for $f'(c)$ gives $u'(c;a)=v'(c;b)$ and $v'(c;a)=-u'(c;b)$, which specialize to the Cauchy–Riemann equations in Cartesian directions.\qed
\section{Gradients and the Chain Rule}

\subsection*{Essential Definitions and Theorems}

\begin{definition}[Gradient]
For a differentiable function $f: \mathbb{R}^n \to \mathbb{R}$, the gradient at a point $x$ is the vector of partial derivatives:
\[\nabla f(x) = \left(\frac{\partial f}{\partial x_1}(x), \ldots, \frac{\partial f}{\partial x_n}(x)\right)\]
\end{definition}

\noindent\textbf{Importance:} The gradient points in the direction of steepest ascent and its magnitude gives the rate of change in that direction. This makes it fundamental for optimization algorithms, understanding the geometry of level sets, and many applications in physics and engineering.



\begin{definition}[Direction of Steepest Ascent]
For a differentiable function $f$ at a point $x$, the direction of steepest ascent is the unit vector in the direction of $\nabla f(x)$, and the maximum directional derivative is $\|\nabla f(x)\|$.
\end{definition}

\noindent\textbf{Importance:} This result follows from the Cauchy-Schwarz inequality and tells us exactly how to move to increase the function value most rapidly. This is the foundation for gradient-based optimization methods and is crucial for understanding the local geometry of functions.



\begin{theorem}[Multivariable Chain Rule]
If $f: \mathbb{R}^n \to \mathbb{R}^m$ is differentiable at $x$ and $g: \mathbb{R}^m \to \mathbb{R}^p$ is differentiable at $f(x)$, then:
\[\nabla(g \circ f)(x) = Df(x)^T \nabla g(f(x))\]
where $Df(x)^T$ is the transpose of the Jacobian matrix.
\end{theorem}

\noindent\textbf{Importance:} This is the gradient version of the chain rule and is essential for computing gradients of composite functions. It's particularly important in machine learning for backpropagation and in optimization for computing gradients of complex objective functions.



\begin{theorem}[Product and Quotient Rules for Gradients]
For differentiable functions $f, g: \mathbb{R}^n \to \mathbb{R}$:
\begin{enumerate}[label=(\alph*)]
\item $\nabla(fg) = f \nabla g + g \nabla f$
\item $\nabla(f/g) = \frac{g \nabla f - f \nabla g}{g^2}$ (where $g \neq 0$)
\end{enumerate}
\end{theorem}

\noindent\textbf{Importance:} These rules allow us to compute gradients of products and quotients of functions, which are common in applications. They extend the familiar rules from single-variable calculus and are essential for gradient computations in optimization and machine learning.



\begin{theorem}[Polar Coordinate Relations]
In $\mathbb{R}^2$, for $r = \sqrt{x^2 + y^2}$ and $\theta = \arctan(y/x)$:
\[\frac{\partial}{\partial x} = \cos \theta \frac{\partial}{\partial r} - \frac{\sin \theta}{r} \frac{\partial}{\partial \theta}\]
\[\frac{\partial}{\partial y} = \sin \theta \frac{\partial}{\partial r} + \frac{\cos \theta}{r} \frac{\partial}{\partial \theta}\]
\end{theorem}

\noindent\textbf{Importance:} These relations allow us to convert between Cartesian and polar coordinates when computing derivatives. They are essential for problems with circular or radial symmetry and appear frequently in physics, engineering, and differential equations.





\begin{problembox}[12.11: Maximum Directional Derivative]
\begin{problemstatement}
Let \( f \) be real-valued and differentiable at a point \( c \) in \( \mathbb{R}^n \), and assume that \( \| \nabla f(c) \| \neq 0 \). Prove that there is one and only one unit vector \( u \) in \( \mathbb{R}^n \) such that \( |f'(c; u)| = \| \nabla f(c) \| \), and that this is the unit vector for which \( |f'(c; u)| \) has its maximum value.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the relation \( f'(c; u) = \nabla f(c) \cdot u \) and apply the Cauchy-Schwarz inequality to find the maximum value. The maximum occurs when \( u \) is parallel to \( \nabla f(c) \).

\bigskip\noindent\textbf{Solution:}
$f'(c;u)=\nabla f(c)\cdot u$. By Cauchy–Schwarz, $|\nabla f\cdot u|\le\|\nabla f\|\,\|u\|=\|\nabla f\|$, with equality iff $u=\pm\dfrac{\nabla f}{\|\nabla f\|}$. The unique unit vector maximizing $f'(c;u)$ is $u=\dfrac{\nabla f}{\|\nabla f\|}$.\qed


\begin{problembox}[12.12: Gradient Calculations]
\begin{problemstatement}
Compute the gradient vector \( \nabla f(x, y) \) at those points \( (x, y) \) in \( \mathbb{R}^2 \) where it exists:
\begin{enumerate}[label=(\alph*)]
\item \( f(x, y) = x^2 y^2 \log (x^2 + y^2) \) if \( (x, y) \ne (0, 0) \), \( f(0, 0) = 0 \).
\item \( f(x, y) = xy \sin \frac{1}{x^2 + y^2} \) if \( (x, y) \ne (0, 0) \), \( f(0, 0) = 0 \).
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Compute partial derivatives using standard differentiation rules. At the origin, check if the function is differentiable by examining the limit definition, since having partial derivatives does not guarantee differentiability.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item For $(x,y)\ne(0,0)$ with $r^2=x^2+y^2$,
\[\nabla f(x,y)=\big(2xy^2\log r^2+\tfrac{2x^3y^2}{r^2},\;2x^2y\log r^2+\tfrac{2x^2y^3}{r^2}\big).\]
At $(0,0)$, $\partial f/\partial x=\partial f/\partial y=0$, but $f$ is not differentiable there; thus $\nabla f$ does not exist at $(0,0)$.
\item For $(x,y)\ne(0,0)$ with $r^2=x^2+y^2$,
\[\nabla f(x,y)=\big(y\sin\tfrac{1}{r^2}-\tfrac{2x^2y}{r^4}\cos\tfrac{1}{r^2},\;x\sin\tfrac{1}{r^2}-\tfrac{2xy^2}{r^4}\cos\tfrac{1}{r^2}\big).\]
At $(0,0)$, the partials are $0$, but $f$ is not differentiable; hence $\nabla f$ does not exist at $(0,0)$.
\end{enumerate}\qed


\begin{problembox}[12.13: Second Order Partials of Composition]
\begin{problemstatement}
Let \( f \) and \( g \) be real-valued functions defined on \( \mathbb{R}^1 \) with continuous second derivatives \( f'' \) and \( g'' \). Define
\[F(x, y) = f[x + g(y)] \text{ for each } (x, y) \text{ in } \mathbb{R}^2.\]
Find formulas for all partials of \( F \) of first and second order in terms of the derivatives of \( f \) and \( g \). Verify the relation
\[(D_1F)(D_{1,2}F) = (D_2F)(D_{1,1}F).\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the chain rule repeatedly to find first and second order partial derivatives. Use the fact that \( F \) depends on \( x \) and \( y \) only through the single variable \( x + g(y) \), which simplifies the calculations.

\bigskip\noindent\textbf{Solution:}
Let $h(x,y)=x+g(y)$. Then 
\[D_1F=f'(h),\quad D_2F=f'(h)g'(y).\] 
For second order: 
$D_{1,1}F=f''(h)$, $D_{1,2}F=f''(h)g'(y)$, $D_{2,2}F=f''(h)[g'(y)]^2+f'(h)g''(y)$. Then $$(D_1F)(D_{1,2}F)=f'(h)f''(h)g'(y)=(D_2F)(D_{1,1}F)$$.\qed


\begin{problembox}[12.14: Polar Coordinate Transformation]
\begin{problemstatement}
Given a function \( f \) defined in \( \mathbb{R}^2 \). Let
\[F(r, \theta) = f(r \cos \theta, r \sin \theta).\]
\begin{enumerate}[label=(\alph*)]
\item Assume appropriate differentiability properties of \( f \) and show that
\begin{align*}
D_1F(r, \theta) =& \cos \theta D_1f(x, y) + \sin \theta D_2f(x, y),\\
D_{1,1}F(r, \theta) =& \cos^2 \theta D_{1,1}f(x, y) + 2 \sin \theta \cos \theta D_{1,2}f(x, y) + \\
& \sin^2 \theta D_{2,2}f(x, y),
\end{align*}
where \( x = r \cos \theta, y = r \sin \theta \).
\item Find similar formulas for \( D_2F, D_{1,2}F, \) and \( D_{2,2}F \).
\item Verify the formula
\[\| \nabla f(r \cos \theta, r \sin \theta) \|^2 = [D_1F(r, \theta)]^2 + \frac{1}{r^2} [D_2F(r, \theta)]^2.\]
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the multivariable chain rule with the coordinate transformation \( x = r \cos \theta, y = r \sin \theta \). For second derivatives, apply the chain rule twice. The gradient formula follows from expressing the Cartesian gradient in polar coordinates.

\bigskip\noindent\textbf{Solution:}
Apply the chain rule with $x=r\cos\theta$, $y=r\sin\theta$. Part (a) follows by differentiating $F(r,\theta)=f(x,y)$ with respect to $r$ twice. Similarly,
\[D_2F=-r\sin\theta\,D_1f+r\cos\theta\,D_2f,\quad D_{1,2}F=-\sin\theta\,D_{1,1}f+\cos\theta\,D_{1,2}f+\cos\theta\,D_{2,1}f+\sin\theta\,D_{2,2}f,\]
and
\[D_{2,2}F=\sin^2\!\theta\,D_{1,1}f-2\sin\theta\cos\theta\,D_{1,2}f+\cos^2\!\theta\,D_{2,2}f.\]
The gradient identity is the standard polar expression $\|\nabla f\|^2=F_r^2+\tfrac{1}{r^2}F_\theta^2$.\qed


\begin{problembox}[12.15: Gradient of Product and Quotient]
\begin{problemstatement}
If \( f \) and \( g \) have gradient vectors \( \nabla f(x) \) and \( \nabla g(x) \) at a point \( x \) in \( \mathbb{R}^n \) show that the product function \( h \) defined by \( h(x) = f(x)g(x) \) also has a gradient vector at \( x \) and that
\[\nabla h(x) = f(x)\nabla g(x) + g(x)\nabla f(x).\]
State and prove a similar result for the quotient \( f/g \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the product rule for differentiation to each component of the gradient. For the quotient, use the quotient rule and express the result in terms of gradients.

\bigskip\noindent\textbf{Solution:}
From the product rule, $\nabla(fg)=f\,\nabla g+g\,\nabla f$. If $g(x)\ne0$, then $\nabla\!\left(\frac{f}{g}\right)=\dfrac{g\,\nabla f-f\,\nabla g}{g^2}$.\qed


\begin{problembox}[12.16: Gradient of Composition]
\begin{problemstatement}
Let \( f \) be a function having a derivative \( f' \) at each point in \( \mathbb{R}^1 \) and let \( g \) be defined on \( \mathbb{R}^3 \) by the equation
\[g(x, y, z) = x^2 + y^2 + z^2.\]
If \( h \) denotes the composite function \( h = f \circ g \), show that
\[\| \nabla h(x, y, z) \|^2 = 4g(x, y, z)[f'[g(x, y, z)]]^2.\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the chain rule to find \( \nabla h = f'(g) \nabla g \), then compute the squared norm using the fact that \( g = x^2 + y^2 + z^2 \).

\bigskip\noindent\textbf{Solution:}
$\nabla h=f'(g)\,\nabla g=f'(g)\,(2x,2y,2z)$, so $\|\nabla h\|^2=4(x^2+y^2+z^2)[f'(g)]^2=4g\,[f'(g)]^2$.\qed


\begin{problembox}[12.17: Gradient of Vector-Valued Composition]
\begin{problemstatement}
Assume \( f \) is differentiable at each point \( (x, y) \) in \( \mathbb{R}^2 \). Let \( g_1 \) and \( g_2 \) be defined on \( \mathbb{R}^3 \) by the equations
\[g_1(x, y, z) = x^2 + y^2 + z^2, \quad g_2(x, y, z) = x + y + z,\]
and let \( g \) be the vector-valued function whose values (in \( \mathbb{R}^2 \)) are given by
\[g(x, y, z) = (g_1(x, y, z), g_2(x, y, z)).\]
Let \( h \) be the composite function \( h = f \circ g \) and show that
\[\| \nabla h \|^2 = 4(D_1f)^2g_1 + 4(D_1f)(D_2f)g_2 + 3(D_2f)^2.\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the multivariable chain rule to express \( \nabla h \) in terms of the partial derivatives of \( f \) and the gradients of \( g_1 \) and \( g_2 \). Then compute the squared norm using the dot product formula.

\bigskip\noindent\textbf{Solution:}
By the chain rule, $\nabla h=(D_1 f)\,\nabla g_1+(D_2 f)\,\nabla g_2$, where $\nabla g_1=(2x,2y,2z)$ and $\nabla g_2=(1,1,1)$. Hence
\begin{align*}
\|\nabla h\|^2&=(D_1 f)^2\|\nabla g_1\|^2+2(D_1 f)(D_2 f)\,\nabla g_1\!\cdot\!\nabla g_2+(D_2 f)^2\|\nabla g_2\|^2\\
&=4(D_1 f)^2 g_1+4(D_1 f)(D_2 f) g_2+3(D_2 f)^2.
\end{align*}\qed


\begin{problembox}[12.18: Euler's Theorem for Homogeneous Functions]
\begin{problemstatement}
Let \( f \) be defined on an open set \( S \) in \( \mathbb{R}^n \). We say that \( f \) is homogeneous of degree \( p \) over \( S \) if \( f(\lambda x) = \lambda^p f(x) \) for every real \( \lambda \) and for every \( x \) in \( S \) for which \( \lambda x \in S \). If such a function is differentiable at \( x \), show that
\[x \cdot \nabla f(x) = p f(x).\]
NOTE. This is known as Euler's theorem for homogeneous functions. Hint. For fixed \( x \), define \( g(\lambda) = f(\lambda x) \) and compute \( g'(1) \).

Also prove the converse. That is, show that if \( x \cdot \nabla f(x) = p f(x) \) for all \( x \) in an open set \( S \), then \( f \) must be homogeneous of degree \( p \) over \( S \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For the forward direction, use the hint to define \( g(\lambda) = f(\lambda x) \) and apply the chain rule to find \( g'(1) \). For the converse, treat the equation as a differential equation and solve it to show homogeneity.

\bigskip\noindent\textbf{Solution:}
If $f(\lambda x)=\lambda^p f(x)$, then with $g(\lambda)=f(\lambda x)$ we have $g'(1)=x\cdot\nabla f(x)=p f(x)$. Conversely, if $x\cdot\nabla f(x)=p f(x)$ on a star-shaped domain, fix $x$ and solve the ODE $\tfrac{d}{d\lambda}f(\lambda x)=x\cdot\nabla f(\lambda x)=p f(\lambda x)$ to get $f(\lambda x)=\lambda^p f(x)$.\qed
\section{Mean-Value Theorems}

\subsection*{Essential Definitions and Theorems}

\begin{definition}[Mean Value Theorem for Multivariable Functions]
If $f: \mathbb{R}^n \to \mathbb{R}$ is differentiable on an open set containing the line segment from $a$ to $b$, then there exists a point $c$ on this line segment such that:
\[f(b) - f(a) = \nabla f(c) \cdot (b - a)\]
\end{definition}

\noindent\textbf{Importance:} This theorem extends the fundamental mean value theorem from single-variable calculus to multivariable functions. It provides a way to estimate function differences in terms of gradients and is essential for proving many results in analysis, optimization, and differential equations.



\begin{definition}[Taylor's Theorem for Multivariable Functions]
If $f: \mathbb{R}^n \to \mathbb{R}$ has continuous partial derivatives up to order $k+1$ in a neighborhood of $a$, then:
\[f(x) = f(a) + \nabla f(a) \cdot (x-a) + \frac{1}{2}(x-a)^T H_f(a)(x-a) + \cdots + R_k(x)\]
where $H_f(a)$ is the Hessian matrix and $R_k(x)$ is the remainder term.
\end{definition}

\noindent\textbf{Importance:} Taylor's theorem provides polynomial approximations of functions near a point. The quadratic term involving the Hessian matrix is particularly important for optimization, as it determines the local curvature and helps classify critical points.



\begin{theorem}[Second Derivative Test]
For a twice-differentiable function $f$ at a critical point $a$ (where $\nabla f(a) = 0$):
\begin{enumerate}[label=(\alph*)]
\item If $H_f(a)$ is positive definite, then $a$ is a local minimum
\item If $H_f(a)$ is negative definite, then $a$ is a local maximum
\item If $H_f(a)$ has both positive and negative eigenvalues, then $a$ is a saddle point
\end{enumerate}
\end{theorem}

\noindent\textbf{Importance:} This test provides a practical way to classify critical points of functions. It's essential for optimization problems and understanding the local geometry of functions. The eigenvalues of the Hessian matrix determine the local behavior.



\begin{theorem}[Implicit Function Theorem]
If $F: \mathbb{R}^{n+m} \to \mathbb{R}^m$ is continuously differentiable near $(a,b)$ with $F(a,b) = 0$ and the Jacobian matrix $D_y F(a,b)$ is invertible, then there exists a function $g: \mathbb{R}^n \to \mathbb{R}^m$ defined near $a$ such that $F(x,g(x)) = 0$ for all $x$ near $a$.
\end{theorem}

\noindent\textbf{Importance:} This theorem tells us when we can solve equations implicitly for some variables in terms of others. It's fundamental for understanding the geometry of level sets and is essential for many applications in differential equations, optimization, and geometry.



\begin{theorem}[Inverse Function Theorem]
If $f: \mathbb{R}^n \to \mathbb{R}^n$ is continuously differentiable near $a$ and the Jacobian matrix $Df(a)$ is invertible, then $f$ has a local inverse near $f(a)$ that is also continuously differentiable.
\end{theorem}

\noindent\textbf{Importance:} This theorem provides conditions under which a function has a local inverse. It's essential for coordinate transformations, solving systems of equations, and understanding the local behavior of differentiable maps.



\noindent\textbf{Tools for this section.} One-variable Mean Value Theorem; mean-value form along line segments $t\mapsto x+t(y-x)$; the vector mean-value identity applied componentwise and after taking dot products.



\begin{problembox}[12.19: Mean-Value Theorem for Vector Functions]
\begin{problemstatement}
Let \( f: \mathbb{R} \rightarrow \mathbb{R}^2 \) be defined by the equation \( f(t) = (\cos t, \sin t) \). Then \( f'(t)(u) = u(-\sin t, \cos t) \) for every real \( u \). The Mean-Value formula
\[f(y) - f(x) = f'(z)(y - x)\]
cannot hold when \( x = 0, y = 2\pi \), since the left member is zero and the right member is a vector of length \( 2\pi \). Nevertheless, Theorem 12.9 states that for every vector \( a \) in \( \mathbb{R}^2 \) there is a \( z \) in the interval \( (0, 2\pi) \) such that
\[a \cdot (f(y) - f(x)) = a \cdot (f'(z)(y - x)).\]
Determine \( z \) in terms of \( a \) when \( x = 0 \) and \( y = 2\pi \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Since \( f(y) - f(x) = 0 \), the equation becomes \( a \cdot f'(z) = 0 \). Use the expression for \( f'(z) \) to find the angle \( z \) that makes this dot product zero.

\bigskip\noindent\textbf{Solution:}
Here $f(y)-f(x)=0$, so $a\cdot f'(z)(2\pi)=0$. Since $f'(z)=(-\sin z,\cos z)$, we require $a_x(-\sin z)+a_y\cos z=0$, i.e., $\tan z=\dfrac{a_y}{a_x}$. Thus $z=\arg(a)\pmod{\pi}$.\qed


\begin{problembox}[12.20: Mean-Value Theorem in Two Variables]
\begin{problemstatement}
Let \( f \) be a real-valued function differentiable on a 2-ball \( B(x) \). By considering the function
\[g(t) = f[ty_1 + (1 - t)x_1, y_2] + f[x_1, ty_2 + (1 - t)x_2]\]
prove that
\[f(y) - f(x) = (y_1 - x_1)D_1f(z_1, y_2) + (y_2 - x_2)D_2f(x_1, z_2),\]
where \( z_1 \in L(x_1, y_1) \) and \( z_2 \in L(x_2, y_2) \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the one-variable Mean Value Theorem to the function \( g(t) \) on the interval \( [0,1] \). Differentiate \( g \) with respect to \( t \) and use the chain rule to express the result in terms of partial derivatives.

\bigskip\noindent\textbf{Solution:}
Apply the 1D MVT to $g$ on $[0,1]$ to get $g'(\theta)=g(1)-g(0)=f(y)-f(x)$. Differentiate $g$ and collect terms to obtain
\[f(y)-f(x)=(y_1-x_1)D_1 f(z_1,y_2)+(y_2-x_2)D_2 f(x_1,z_2)\]
for some $z_1\in L(x_1,y_1)$, $z_2\in L(x_2,y_2)$.\qed


\begin{problembox}[12.21: Generalized Mean-Value Theorem]
\begin{problemstatement}
State and prove a generalization of the result in Exercise 12.20 for a real-valued function differentiable on an \( n \)-ball \( B(x) \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Generalize the approach from Exercise 12.20 by constructing a function that varies one coordinate at a time. Apply the one-variable Mean Value Theorem to this function and use the chain rule to express the result in terms of partial derivatives.

\bigskip\noindent\textbf{Solution:}
For $f$ differentiable on a convex $B(x)$ and $y\in B(x)$, there exist $\xi_k\in L(x_k,y_k)$ such that
\[f(y)-f(x)=\sum_{k=1}^n (y_k-x_k)\,D_k f(x_1,\dots,\xi_k,\dots,y_n).\]
Proof: define $g(t)=\sum_{k=1}^n f(x_1,\dots,tx_k+(1-t)y_k,\dots,y_n)$ and apply the 1D MVT as in 12.20.\qed


\begin{problembox}[12.22: Mean-Value Theorem for Directional Derivatives]
\begin{problemstatement}
Let \( f \) be real-valued and assume that the directional derivative \( f'(c + tu; u) \) exists for each \( t \) in the interval \( 0 \leq t \leq 1 \). Prove that for some \( \theta \) in the open interval \( (0, 1) \) we have
\[f(c + u) - f(c) = f'(c + \theta u; u).\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Define a one-variable function \( h(t) = f(c + tu) \) and apply the one-variable Mean Value Theorem to it on the interval \( [0,1] \). The derivative of \( h \) is the directional derivative of \( f \).

\bigskip\noindent\textbf{Solution:}
Apply the 1D MVT to $h(t)=f(c+tu)$ on $[0,1]$: $f(c+u)-f(c)=h'(\theta)=f'(c+\theta u;u)$ for some $\theta\in(0,1)$.\qed


\begin{problembox}[12.23: Zero Directional Derivatives]
\begin{problemstatement}
\begin{enumerate}[label=(\alph*)]
\item If \( f \) is real-valued and if the directional derivative \( f'(x; u) = 0 \) for every \( x \) in an \( n \)-ball \( B(c) \) and every direction \( u \), prove that \( f \) is constant on \( B(c) \).
\item What can you conclude about \( f \) if \( f'(x; u) = 0 \) for a fixed direction \( u \) and every \( x \) in \( B(c) \)?
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For part (a), use the fact that zero directional derivatives in all directions imply zero gradient, then integrate along paths to show constancy. For part (b), consider what happens when moving only in the direction \( u \).

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item If $f'(x;u)=0$ for all $u$, then $\nabla f(x)=0$ wherever $f$ is differentiable; integrating $\nabla f$ along any path in $B(c)$ gives $f\equiv\text{constant}$.
\item $f$ is constant along every line parallel to $u$; that is, $t\mapsto f(x+tu)$ is constant for each $x$.
\end{enumerate}\qed
\section{Derivatives of Higher Order and Taylor's Formula}

\subsection*{Essential Definitions and Theorems}

\begin{definition}[Higher-Order Partial Derivatives]
For a function $f: \mathbb{R}^n \to \mathbb{R}$, the $k$-th order partial derivative with respect to variables $i_1, i_2, \ldots, i_k$ is defined as:
\[D_{i_1, i_2, \ldots, i_k} f = D_{i_1}(D_{i_2}(\cdots(D_{i_k} f)\cdots))\]
\end{definition}

\noindent\textbf{Importance:} Higher-order partial derivatives measure how functions change at increasingly fine scales. They are essential for Taylor series expansions, optimization (via the Hessian matrix), and understanding the local behavior of functions. The order and arrangement of these derivatives determine the structure of polynomial approximations.



\begin{definition}[Hessian Matrix]
For a twice-differentiable function $f: \mathbb{R}^n \to \mathbb{R}$, the Hessian matrix at a point $x$ is the $n \times n$ matrix of second partial derivatives:
\[H_f(x) = \begin{bmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}\]
\end{definition}

\noindent\textbf{Importance:} The Hessian matrix captures the local curvature of a function and is crucial for understanding the behavior near critical points. Its eigenvalues determine whether a critical point is a local minimum, maximum, or saddle point. The Hessian is also essential for Newton's method and other optimization algorithms.



\begin{theorem}[Clairaut's Theorem (Equality of Mixed Partials)]
If $f: \mathbb{R}^n \to \mathbb{R}$ has continuous second partial derivatives in a neighborhood of a point, then the mixed partial derivatives are equal:
\[D_{i,j} f = D_{j,i} f\]
for all $i, j$.
\end{theorem}

\noindent\textbf{Importance:} This theorem shows that the order of differentiation doesn't matter for continuous functions, which greatly simplifies calculations and proofs. It's essential for the symmetry of the Hessian matrix and for many results in differential equations and analysis.



\begin{theorem}[Taylor's Formula for Multivariable Functions]
If $f: \mathbb{R}^n \to \mathbb{R}$ has continuous partial derivatives up to order $k+1$ in a neighborhood of $a$, then:
\[f(x) = \sum_{|\alpha| \leq k} \frac{D^\alpha f(a)}{\alpha!}(x-a)^\alpha + R_k(x)\]
where $\alpha$ is a multi-index, $D^\alpha$ is the corresponding partial derivative, and $R_k(x)$ is the remainder term.
\end{theorem}

\noindent\textbf{Importance:} Taylor's formula provides polynomial approximations of functions near a point, which are essential for understanding local behavior, optimization, and numerical methods. The remainder term gives precise control over the approximation error, making this theorem fundamental for analysis and applications.





\begin{problembox}[12.24: Equality of Mixed Partials]
\begin{problemstatement}
For each of the following functions, verify that the mixed partial derivatives \( D_{1,2}f \) and \( D_{2,1}f \) are equal.
\begin{enumerate}[label=(\alph*)]
\item \( f(x, y) = x^4 + y^4 - 4x^2y^2 \).
\item \( f(x, y) = \log (x^2 + y^2) \), \( (x, y) \neq (0, 0) \).
\item \( f(x, y) = \tan (x^2/y) \), \( y \neq 0 \).
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use Clairaut's theorem which states that if the mixed partial derivatives are continuous, then they are equal. Alternatively, compute both mixed partials directly and verify they are identical.

\bigskip\noindent\textbf{Solution:}
All listed functions are $C^2$ on their stated domains; by Clairaut's theorem, $D_{1,2}f=D_{2,1}f$ there. Direct computation also confirms the equality.\qed


\begin{problembox}[12.25: Equality of Higher-Order Mixed Partials]
\begin{problemstatement}
Let \( f \) be a function of two variables. Use induction and Theorem 12.13 to prove that if the \( 2^k \) partial derivatives of \( f \) of order \( k \) are continuous in a neighborhood of a point \( (x, y) \), then all mixed partials of the form \( D_{r_1, \ldots, r_k} f \) and \( D_{p_1, \ldots, p_k} f \) will be equal at \( (x, y) \) if the \( k \)-tuple \( (r_1, \ldots, r_k) \) contains the same number of ones as the \( k \)-tuple \( (p_1, \ldots, p_k) \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use mathematical induction on the order \( k \). The base case \( k = 2 \) is Clairaut's theorem. For the inductive step, use the fact that any permutation can be achieved by swapping adjacent elements, and each swap is allowed by the \( k = 2 \) case.

\bigskip\noindent\textbf{Solution:}
For $k=2$ this is Clairaut's theorem. Assume the statement for order $k$. For order $k+1$, use the fact that any permutation can be achieved by swapping adjacent derivatives two at a time using the $k=2$ case (continuity ensures commutation), to reorder any arrangement into any other with the same number of 1's and 2's. Thus all mixed partials of the same type coincide.\qed


\begin{problembox}[12.26: Taylor's Formula for Two Variables]
\begin{problemstatement}
If \( f \) is a function of two variables having continuous partials of order \( k \) on some open set \( S \) in \( \mathbb{R}^2 \), show that
\[f^{(k)} (x; t) = \sum_{r=0}^{k} \binom{k}{r} t_1^r t_2^{k-r} D_{p_1}, \ldots, p_k f(x), \quad \text{if } x \in S, \quad t = (t_1, t_2),\]
where in the \( r \)th term we have \( p_1 = \cdots = p_r = 1 \) and \( p_{r+1} = \cdots = p_k = 2 \). Use this result to give an alternative expression for Taylor's formula (Theorem 12.14) in the case when \( n = 2 \). The symbol \( \binom{k}{r} \) is the binomial coefficient \( k! / [r! (k - r)!] \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the multilinearity of the \( k \)th derivative and the fact that mixed partials of the same type are equal. The binomial coefficients arise from the number of ways to choose \( r \) derivatives with respect to the first variable and \( k-r \) with respect to the second.

\bigskip\noindent\textbf{Solution:}
By multilinearity and symmetry of mixed partials,
\[f^{(k)}(x;t)=\sum_{r=0}^k \binom{k}{r} t_1^r t_2^{k-r} D_{\underbrace{1,\dots,1}_{r},\underbrace{2,\dots,2}_{k-r}} f(x).\]
Hence, for $h=(h_1,h_2)$,
\begin{align*}
f(x+h)&=\sum_{j=0}^k \frac{1}{j!}\,f^{(j)}(x;h)+R_{k+1}(x,h)\\
&=\sum_{j=0}^k\sum_{r=0}^j \frac{1}{r!(j-r)!} D_{\!\underbrace{1,\dots,1}_{r},\underbrace{2,\dots,2}_{j-r}} f(x)\,h_1^{\,r} h_2^{\,j-r}+R_{k+1},
\end{align*}
which is the two-variable Taylor polynomial written by grouping powers of $h_1,h_2$.\qed


\begin{problembox}[12.27: Taylor Expansion]
\begin{problemstatement}
Use Taylor's formula to express the following in powers of \( (x - 1) \) and \( (y - 2) \):
\begin{enumerate}[label=(\alph*)]
\item \( f(x, y) = x^3 + y^3 + xy^2 \),
\item \( f(x, y) = x^2 + xy + y^2 \).
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Make the substitution \( \alpha = x - 1 \) and \( \beta = y - 2 \), then expand each term using the binomial theorem. Collect terms by powers of \( \alpha \) and \( \beta \).

\bigskip\noindent\textbf{Solution:}
Let $\alpha=x-1$, $\beta=y-2$.
\begin{enumerate}[label=(\alph*)]
\item $(1+\alpha)^3+(2+\beta)^3+(1+\alpha)(2+\beta)^2=13+7\alpha+16\beta+3\alpha^2+4\alpha\beta+7\beta^2+\alpha^3+\beta^3+\alpha\beta^2$.
\item $(1+\alpha)^2+(1+\alpha)(2+\beta)+(2+\beta)^2=7+4\alpha+5\beta+\alpha^2+\alpha\beta+\beta^2$.
\end{enumerate}

\begin{techniquessection}[Solving and Proving Techniques]

\subsection*{Proving Differentiability}
\begin{itemize}
\item Use the definition: $f$ is differentiable at $c$ if there exists a linear map $Df(c)$ such that $f(c+h) = f(c) + Df(c)h + o(\|h\|)$
\item Show that all partial derivatives exist and are continuous
\item Use the fact that continuously differentiable functions are differentiable
\item Apply the fact that differentiable functions are continuous
\item Use the fact that vector-valued functions are differentiable if and only if each component is differentiable
\end{itemize}

\subsection*{Working with Partial Derivatives}
\begin{itemize}
\item Use the definition: $D_k f(x) = \lim_{h \to 0} \frac{f(x + he_k) - f(x)}{h}$
\item Apply the fact that partial derivatives are computed by treating other variables as constants
\item Use the fact that partial derivatives commute under continuity (Clairaut's theorem)
\item Apply the fact that directional derivatives can be computed from partial derivatives: $f'(x;u) = \nabla f(x) \cdot u$
\item Use the fact that partial derivatives are linear operators
\end{itemize}

\subsection*{Proving Local Extrema}
\begin{itemize}
\item Use the fact that if $f$ has a local extremum at $c$, then all partial derivatives $D_k f(c) = 0$
\item Apply the second derivative test using the Hessian matrix
\item Use the fact that critical points are where the gradient vanishes
\item Apply the fact that local extrema occur at critical points or boundary points
\item Use the fact that continuous functions on compact sets attain their extrema
\end{itemize}

\subsection*{Working with Directional Derivatives}
\begin{itemize}
\item Use the definition: $f'(x;u) = \lim_{t \to 0} \frac{f(x + tu) - f(x)}{t}$
\item Apply the fact that directional derivatives are linear in the direction: $f'(x;cu) = cf'(x;u)$
\item Use the fact that if $f$ is differentiable, then $f'(x;u) = \nabla f(x) \cdot u$
\item Apply the fact that zero directional derivatives in all directions imply the function is constant
\item Use the fact that directional derivatives can be used to find the direction of steepest ascent
\end{itemize}

\subsection*{Applying the Mean Value Theorem}
\begin{itemize}
\item Use the one-variable MVT by restricting to lines: $f(c+u) - f(c) = f'(c+\theta u;u)$ for some $\theta \in (0,1)$
\item Apply the fact that the MVT can be used to bound function differences
\item Use the fact that the MVT can be generalized to multiple variables using convexity
\item Apply the fact that the MVT can be used to prove constancy results
\item Use the fact that the MVT can be used to establish Lipschitz conditions
\end{itemize}

\subsection*{Working with Taylor's Formula}
\begin{itemize}
\item Use the fact that Taylor's formula provides polynomial approximations of functions
\item Apply the fact that Taylor polynomials can be computed using partial derivatives
\item Use the fact that the remainder term can be bounded using higher-order derivatives
\item Apply the fact that Taylor series can be used to approximate functions near a point
\item Use the fact that Taylor's formula can be used to prove differentiability results
\end{itemize}
\end{techniquessection}