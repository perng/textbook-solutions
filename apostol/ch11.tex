\chapter{Fourier Series and Fourier Integrals}

\section{Orthogonal Systems}

\subsection*{Essential Definitions and Theorems}

\begin{definition}[Orthogonal System]
A collection of functions $\{\varphi_n\}$ defined on an interval $[a, b]$ is called an orthogonal system if $\langle \varphi_m, \varphi_n \rangle = 0$ for all $m \neq n$, where $\langle f, g \rangle = \int_a^b f(x) \overline{g(x)} \, dx$ is the inner product. If in addition $\|\varphi_n\| = 1$ for all $n$, the system is called orthonormal.
\end{definition}

\noindent\textbf{Importance:} Orthogonal systems provide the foundation for Fourier analysis and many other areas of mathematics. They allow us to decompose complex functions into simpler components and are essential for understanding function spaces. The orthogonality property makes calculations much simpler and provides geometric intuition for function approximation.



\begin{definition}[Fourier Coefficients]
For a function $f$ and an orthonormal system $\{\varphi_n\}$, the Fourier coefficients are defined as $c_n = \langle f, \varphi_n \rangle = \int_a^b f(x) \overline{\varphi_n(x)} \, dx$. The Fourier series of $f$ is the formal sum $\sum_{n=0}^{\infty} c_n \varphi_n(x)$.
\end{definition}

\noindent\textbf{Importance:} Fourier coefficients provide the best approximation of a function in terms of the given orthogonal system. They encode the "frequency content" of the function and are essential for understanding how functions can be represented as infinite series. The study of convergence of Fourier series is central to harmonic analysis.



\begin{theorem}[Bessel's Inequality]
For any function $f$ and orthonormal system $\{\varphi_n\}$, we have $\sum_{n=0}^{\infty} |c_n|^2 \leq \|f\|^2$, where $c_n$ are the Fourier coefficients of $f$.
\end{theorem}

\noindent\textbf{Importance:} Bessel's inequality provides a fundamental bound on the Fourier coefficients and ensures that the series $\sum |c_n|^2$ converges. This is essential for the convergence theory of Fourier series and provides a measure of how well a function can be approximated by its Fourier series. It's the foundation for many convergence results.



\begin{theorem}[Parseval's Identity]
If $\{\varphi_n\}$ is a complete orthonormal system and $f$ is square-integrable, then $\sum_{n=0}^{\infty} |c_n|^2 = \|f\|^2$, where $c_n$ are the Fourier coefficients of $f$.
\end{theorem}

\noindent\textbf{Importance:} Parseval's identity is one of the most important results in Fourier analysis. It shows that the Fourier transform preserves the "energy" of the function and provides a precise relationship between the function and its Fourier coefficients. This identity is crucial for many applications in physics and engineering.



\begin{theorem}[Gram-Schmidt Orthogonalization]
Given a linearly independent system $\{f_n\}$, there exists an orthonormal system $\{\varphi_n\}$ such that the span of $\{\varphi_0, \ldots, \varphi_n\}$ equals the span of $\{f_0, \ldots, f_n\}$ for each $n$.
\end{theorem}

\noindent\textbf{Importance:} The Gram-Schmidt process provides a systematic way to construct orthonormal bases from any linearly independent system. This is essential for many applications where we need to work with orthogonal functions but only have access to non-orthogonal ones. It's a fundamental tool in linear algebra and functional analysis.



\begin{problembox}[11.1: Orthonormality of Trigonometric System]
\begin{problemstatement}
Verify that the trigonometric system in (1) is orthonormal on $[0, 2\pi]$.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the definition of orthonormality and compute the inner products $\langle \varphi_m, \varphi_n \rangle$ directly using trigonometric identities and integration by parts to show they equal $\delta_{mn}$.

\bigskip\noindent\textbf{Solution:}
Let $\langle f, g\rangle = \int_0^{2\pi} f(x)\,\overline{g(x)}\,dx$. For integers $m, n \ge 1$,
\begin{align*}
\int_0^{2\pi} \cos(mx)\cos(nx)\,dx &= \pi\,\delta_{mn},\\
\int_0^{2\pi} \sin(mx)\sin(nx)\,dx &= \pi\,\delta_{mn},\\
\int_0^{2\pi} \cos(mx)\sin(nx)\,dx &= 0,
\end{align*}
and $\int_0^{2\pi} 1\cdot 1\,dx = 2\pi$, $\int_0^{2\pi} 1\cdot \cos(nx)\,dx = \int_0^{2\pi} 1\cdot \sin(nx)\,dx = 0$. With the normalizing factors specified in (1), these give $\langle \varphi_m, \varphi_n\rangle = \delta_{mn}$, hence orthonormality on $[0,2\pi]$.\qed


\begin{problembox}[11.2: Linear Independence of Orthonormal Systems]
\begin{problemstatement}
A finite collection of functions $\{\varphi_0, \varphi_1, \dots, \varphi_M\}$ is said to be linearly independent on $[a, b]$ if the equation
\[
\sum_{k=0}^M c_k \varphi_k(x) = 0
\]
for all $x$ in $[a, b]$ implies $c_0 = c_1 = \dots = c_M = 0$. An infinite collection is called linearly independent on $[a, b]$ if every finite subset is linearly independent on $[a, b]$. Prove that every orthonormal system on $[a, b]$ is linearly independent on $[a, b]$.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the orthonormality property to take inner products with each basis function, which will isolate individual coefficients and force them to be zero.

\bigskip\noindent\textbf{Solution:}
Suppose $\sum_{k=0}^M c_k\,\varphi_k(x)=0$ for all $x\in[a,b]$. Taking inner products with $\varphi_j$ and using orthonormality gives
\[
0 = \Big\langle \sum_{k=0}^M c_k\,\varphi_k,\,\varphi_j\Big\rangle = \sum_{k=0}^M c_k\,\langle\varphi_k,\varphi_j\rangle = c_j\quad (j=0,\dots,M).
\]
Hence all $c_j=0$ and the set is linearly independent. For an infinite set, every finite subset is orthonormal, hence linearly independent.\qed


\begin{problembox}[11.3: Gram-Schmidt Orthogonalization]
\begin{problemstatement}
Let $\{f_0, f_1, \dots\}$ be a linearly independent system on $[a, b]$ (as defined in Exercise 11.2). Define a new system $\{g_0, g_1, \dots\}$ recursively as follows:
\[
g_0 = f_0, \quad g_{n+1} = f_{n+1} - \sum_{k=0}^n a_k g_k,
\]
where $a_k = (f_{n+1}, g_k)/(g_k, g_k)$ if $\|g_k\| \neq 0$, and $a_k = 0$ if $\|g_k\| = 0$. Prove that $g_{n+1}$ is orthogonal to each of $g_0, g_1, \dots, g_n$ for every $n \geq 0$.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the recursive definition of $g_{n+1}$ and compute its inner product with each $g_j$ for $j \leq n$, using the choice of coefficients $a_k$ to ensure orthogonality.

\bigskip\noindent\textbf{Solution:}
For $0\le j\le n$,
\[
\langle g_{n+1}, g_j\rangle = \Big\langle f_{n+1} - \sum_{k=0}^n a_k g_k,\, g_j\Big\rangle = \langle f_{n+1}, g_j\rangle - a_j\langle g_j, g_j\rangle = 0,
\]
since $a_j = \langle f_{n+1}, g_j\rangle/\langle g_j, g_j\rangle$ when $\|g_j\|\neq 0$, and $a_j=0$ when $\|g_j\|=0$. Thus $g_{n+1}\perp g_j$ for each $j\le n$.\qed


\begin{problembox}[11.4: Gram-Schmidt on Polynomials]
\begin{problemstatement}
Let $(f, g) = \int_{-1}^1 f(t)g(t) \, dt$. Apply the Gram-Schmidt process to the system of polynomials $\{1, t, t^2, \dots\}$ on the interval $[-1, 1]$ and show that
\[
g_1(t) = t, \quad g_2(t) = t^2 - \frac{1}{3}, \quad g_3(t) = t^3 - \frac{3}{5}t, \quad g_4(t) = t^4 - \frac{6}{7}t^2 + \frac{3}{35}.
\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the Gram-Schmidt process step by step, computing the necessary inner products and normalization factors to construct each orthogonal polynomial.

\bigskip\noindent\textbf{Solution:}
With $(f,g)=\int_{-1}^1 f(t)g(t)\,dt$ and $g_0=1$, orthogonalize:
\[
g_1 = t - \frac{\langle t,1\rangle}{\langle 1,1\rangle} = t,\quad
g_2 = t^2 - \frac{\langle t^2,1\rangle}{\langle 1,1\rangle} = t^2 - \frac{1}{3},
\]
since $\int_{-1}^1 t\,dt=0$ and $\int_{-1}^1 t^2\,dt=2/3$. Continuing,
\[
g_3 = t^3 - \alpha t,\ \ \alpha = \frac{\langle t^3, t\rangle}{\langle t,t\rangle} = \frac{\int_{-1}^1 t^4\,dt}{\int_{-1}^1 t^2\,dt} = \frac{2/5}{2/3} = \frac{3}{5},
\]
and similarly $g_4 = t^4 - \beta t^2 + \gamma$ with $\beta=6/7$, $\gamma=3/35$. These match the listed formulas.\qed


\begin{problembox}[11.5: Approximation of Periodic Functions]
\begin{problemstatement}
\begin{enumerate}[label=(\alph*)]
\item Assume $f \in \mathcal{R}$ on $[0, 2\pi]$, where $f$ is real and has period $2\pi$. Prove that for every $\epsilon > 0$, there is a continuous function $g$ of period $2\pi$ such that $\|f - g\| < \epsilon$.
\begin{itemize}
\item \textit{Hint:} Choose a partition $P$ of $[0, 2\pi]$ for which $f$ satisfies Riemann's condition $U(P, f) - L(P, f) < \epsilon$ and construct a piecewise linear $g$ which agrees with $f$ at the points of $P$.
\end{itemize}
\item Use part (a) to show that Theorem 11.16(a), (b), and (c) holds if $f$ is Riemann integrable on $[0, 2\pi]$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For part (a), use Riemann's condition to construct a piecewise linear approximation. For part (b), use the density result from (a) and the fact that Fourier coefficients depend continuously on the function in $L^2$.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item If $f\in\mathcal R[0,2\pi]$ is $2\pi$-periodic and real, choose a partition $P$ with $U(P,f)-L(P,f)<\epsilon$. Define $g$ piecewise linear joining the points $\{(x_i,f(x_i))\}_{x_i\in P}$ and extend $2\pi$-periodically. Then $g$ is continuous and $\|f-g\|^2=\int_0^{2\pi}|f-g|^2<\epsilon\,(U-L)$ can be made $<\epsilon^2$ by refining $P$, so $\|f-g\|<\epsilon$.
\item The conclusions of Theorem 11.16 hold for continuous periodic $g$. Given $f\in\mathcal R$, pick continuous periodic $g$ with $\|f-g\|<\epsilon$. Apply the theorem to $g$ and pass to $f$ by triangle and Cauchy–Schwarz inequalities; the Fourier coefficients depend continuously on $f$ in $L^2$, so the statements (a),(b),(c) extend to Riemann integrable $f$.
\end{enumerate}\qed


\begin{problembox}[11.6: Completeness of Orthonormal Systems]
\begin{problemstatement}
In this exercise, all functions are assumed to be continuous on a compact interval $[a, b]$. Let $\{\varphi_0, \varphi_1, \dots\}$ be an orthonormal system on $[a, b]$.
\begin{enumerate}[label=(\alph*)]
\item Prove that the following three statements are equivalent:
\begin{enumerate}[label=\arabic*)]
\item $(f, \varphi_n) = (g, \varphi_n)$ for all $n$ implies $f = g$. (Two distinct continuous functions cannot have the same Fourier coefficients.)
\item $(f, \varphi_n) = 0$ for all $n$ implies $f = 0$. (The only continuous function orthogonal to every $\varphi_n$ is the zero function.)
\item If $T$ is an orthonormal set on $[a, b]$ such that $\{\varphi_0, \varphi_1, \dots\} \subseteq T$, then $\{\varphi_0, \varphi_1, \dots\} = T$. (We cannot enlarge the orthonormal set.) This property is described by saying that $\{\varphi_0, \varphi_1, \dots\}$ is maximal or complete.
\end{enumerate}
\item Let $\varphi_n(x) = e^{inx}/\sqrt{2\pi}$ for $n$ an integer, and verify that the set $\{\varphi_n : n \in \mathbb{Z}\}$ is complete on every interval of length $2\pi$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For part (a), show the implications in a cycle: (1)$\Rightarrow$(2)$\Rightarrow$(3)$\Rightarrow$(1). For part (b), use Fejér's theorem on uniform convergence of Cesàro means to show that the only function orthogonal to all exponentials is the zero function.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item (1)$\Rightarrow$(2): take $g=0$. (2)$\Rightarrow$(3): if $T\supsetneq\{\varphi_n\}$ is orthonormal, pick $\psi\in T\setminus\{\varphi_n\}$. Then $\psi$ is orthogonal to each $\varphi_n$, so by (2) we must have $\psi=0$, a contradiction. (3)$\Rightarrow$(1): if $\langle f- g,\varphi_n\rangle=0$ for all $n$, then the orthonormal set $\{\varphi_n\}$ is properly contained in the orthonormal set $\{\varphi_n\}\cup\{\tfrac{f-g}{\|f-g\|}\}$ unless $f=g$. Thus $f=g$.
\item For $\varphi_n(x)=e^{inx}/\sqrt{2\pi}$ on any interval of length $2\pi$, if $f$ is continuous and $\langle f,\varphi_n\rangle=0$ for all $n\in\mathbb Z$, Fejér's theorem gives uniform convergence of Cesàro means of the Fourier series to $f$. These means vanish, hence $f\equiv 0$. By part (a)(2), the system is complete.
\end{enumerate}\qed


\begin{problembox}[11.7: Properties of Legendre Polynomials]
\begin{problemstatement}
If $x \in \mathbb{R}$ and $n = 1, 2, \dots$, let $f_n(x) = (x^2 - 1)^n$ and define
\[
\varphi_0(x) = 1, \quad \varphi_n(x) = \frac{1}{2^{n}n!} f_n^{(n)}(x).
\]
It is clear that $\varphi_n$ is a polynomial. This is called the Legendre polynomial of order $n$. The first few are
\[
\begin{aligned}
\varphi_1(x) &= x, \\
\varphi_2(x) &= \frac{1}{2}(3x^2 - 1), \\
\varphi_3(x) &= \frac{1}{2}(5x^3 - 3x), \\
\varphi_4(x) &= \frac{1}{8}(35x^4 - 30x^2 + 3).
\end{aligned}
\]
Derive the following properties of Legendre polynomials:
\begin{enumerate}[label=(\alph*)]
\item $\varphi_n'(x) = x \, \varphi_{n-1}'(x) + n \, \varphi_{n-1}(x)$.
\item $\varphi_n(x) = x \, \varphi_{n-1}(x) + \frac{x^2 - 1}{n} \, \varphi_{n-1}'(x)$.
\item $(n + 1) \varphi_{n+1}(x) = (2n + 1) x \varphi_n(x) - n \varphi_{n-1}(x)$.
\item $\varphi_n$ satisfies the differential equation $[(1 - x^2) y']' + n(n + 1) y = 0$.
\item $[(1 - x^2) \Delta(x)]' + [m(m + 1) - n(n + 1)] \varphi_m(x) \varphi_n(x) = 0$, where $\Delta = \varphi_n \varphi_m' - \varphi_m \varphi_n'$.
\item The set $\{\varphi_0, \varphi_1, \varphi_2, \dots\}$ is orthogonal on $[-1, 1]$.
\item $\int_{-1}^1 \varphi_n^2(x) \, dx = \frac{2n - 1}{2n + 1} \int_{-1}^1 \varphi_{n-1}^2(x) \, dx$.
\item $\int_{-1}^1 \varphi_n^2(x)\, dx = \frac{2}{2n+1}$.
\end{enumerate}
\textit{Note:} The polynomials $g_n(t) = \sqrt{\frac{2n + 1}{2}} \varphi_n(t)$ arise by applying the Gram-Schmidt process to the system $\{1, t, t^2, \dots\}$ on the interval $[-1, 1]$. (See Exercise 11.4.)
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use Rodrigues' formula and Leibniz' rule for derivatives to establish the recurrence relations. Use the differential equation to prove orthogonality, and integrate the recurrence relations to find normalization constants.

\bigskip\noindent\textbf{Solution:}
Use Rodrigues' formula
\[
\varphi_n(x)=\frac{1}{2^{n} n!}\,\frac{d^n}{dx^n}(x^2-1)^n,\quad n\ge0,\qquad \varphi_0\equiv1,\ \varphi_n(1)=1.
\]
\begin{enumerate}[label=(\alph*)]
\item Let $f_n=(x^2-1)^n=(x^2-1)f_{n-1}$. By Leibniz' rule on $f_n^{(n+1)}$ and the definition of $\varphi_k$, one obtains
\[
\varphi_n'(x)=x\,\varphi_{n-1}'(x)+n\,\varphi_{n-1}(x).
\]
\item Integrate (a): $\dfrac{d}{dx}\big(\varphi_n - x\varphi_{n-1}\big) = \dfrac{x^2-1}{n}\,\varphi_{n-1}'' + \dfrac{2x}{n}\,\varphi_{n-1}' = \dfrac{d}{dx}\Big( \frac{x^2-1}{n}\,\varphi_{n-1}' \Big)$. Using $\varphi_k(1)=1$ fixes the constant, giving
\[
\varphi_n(x)=x\,\varphi_{n-1}(x)+\frac{x^2-1}{n}\,\varphi_{n-1}'(x).
\]
\item Eliminate $\varphi_{n-1}'$ between (a) and (b), or differentiate (b) and use (a), to get Bonnet's three-term recurrence
\[
(n+1)\,\varphi_{n+1}(x)=(2n+1)\,x\,\varphi_n(x)-n\,\varphi_{n-1}(x).
\]
\item Differentiate (b) and use (c) to eliminate $\varphi_{n\pm1}$, which yields Legendre's ODE
\[
\big[(1-x^2)y'\big]' + n(n+1)\,y=0\quad (y=\varphi_n).
\]
\item Let $\Delta=\varphi_n\varphi_m'-\varphi_m\varphi_n'$. Subtract the equations in (d) for $m$ and $n$ to obtain
\[
\big[(1-x^2)\Delta(x)\big]'+\big(m(m+1)-n(n+1)\big)\,\varphi_m(x)\,\varphi_n(x)=0.
\]
\item Integrate (e) over $[-1,1]$. The boundary term vanishes since $1-x^2=0$ at $\pm1$. For $m\ne n$ this gives $\int_{-1}^1\varphi_m\varphi_n\,dx=0$, so $\{\varphi_n\}$ is orthogonal on $[-1,1]$.
\item Multiply (c) with index $n-1$ by $\varphi_n$ and integrate; by orthogonality only $\int x\,\varphi_{n-1}\varphi_n$ survives. Do the same with (c) at index $n$ times $\varphi_{n-1}$ and compare. The result is
\[
\int_{-1}^1 \varphi_n(x)^2\,dx=\frac{2n-1}{2n+1}\int_{-1}^1 \varphi_{n-1}(x)^2\,dx.
\]
\item Using (g) and $\int_{-1}^1\varphi_0^2\,dx=2$, induction yields
\[
\int_{-1}^1 \varphi_n(x)^2\,dx=\frac{2}{2n+1}.
\]
\end{enumerate}\qed
\section{Trigonometric Fourier Series}

\subsection*{Essential Definitions and Theorems}

\begin{definition}[Fourier Series]
For a function $f \in L([-\pi, \pi])$, the Fourier series of $f$ is defined as:
\[f(x) \sim \frac{a_0}{2} + \sum_{n=1}^{\infty} (a_n \cos nx + b_n \sin nx)\]
where the Fourier coefficients are:
\[a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(t) \cos nt \, dt, \quad b_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(t) \sin nt \, dt\]
\end{definition}

\noindent\textbf{Importance:} Fourier series provide a way to represent periodic functions as infinite sums of trigonometric functions. This is one of the most powerful tools in analysis, allowing us to decompose complex periodic phenomena into simple harmonic components. The coefficients encode the frequency content of the function.



\begin{definition}[Orthogonality of Trigonometric Functions]
The functions $\{1, \cos nx, \sin nx : n \in \mathbb{N}\}$ form an orthogonal system on $[-\pi, \pi]$ with respect to the inner product $\langle f, g \rangle = \int_{-\pi}^{\pi} f(x) g(x) \, dx$.
\end{definition}

\noindent\textbf{Importance:} Orthogonality is the key property that makes Fourier series work. It allows us to compute coefficients independently and ensures that the series representation is unique. This orthogonality property is what makes trigonometric functions the natural choice for representing periodic functions.



\begin{theorem}[Parseval's Identity]
If $f \in L^2([-\pi, \pi])$ has Fourier series $f(x) \sim \frac{a_0}{2} + \sum_{n=1}^{\infty} (a_n \cos nx + b_n \sin nx)$, then:
\[\frac{1}{\pi} \int_{-\pi}^{\pi} |f(x)|^2 \, dx = \frac{|a_0|^2}{2} + \sum_{n=1}^{\infty} (|a_n|^2 + |b_n|^2)\]
\end{theorem}

\noindent\textbf{Importance:} Parseval's identity shows that the energy of a signal (the $L^2$ norm) is preserved in the frequency domain. This is crucial for applications in signal processing and physics, where energy conservation is fundamental. It also provides a way to test convergence of Fourier series.



\begin{theorem}[Convergence of Fourier Series]
\begin{enumerate}[label=(\alph*)]
\item If $f$ is continuous and piecewise smooth on $[-\pi, \pi]$, then its Fourier series converges uniformly to $f$.
\item If $f$ is piecewise continuous with piecewise continuous derivative, then the Fourier series converges pointwise to $\frac{f(x^+) + f(x^-)}{2}$.
\item If $f \in L^2([-\pi, \pi])$, then the Fourier series converges to $f$ in $L^2$ norm.
\end{enumerate}
\end{theorem}

\noindent\textbf{Importance:} These convergence results tell us when and how Fourier series represent the original function. The different types of convergence (uniform, pointwise, $L^2$) are important for different applications. The Gibbs phenomenon at discontinuities is a key feature that appears in applications.





\begin{problembox}[11.8: Fourier Series for Even and Odd Functions]
\begin{problemstatement}
Assume that $f \in L([-\pi, \pi])$ and that $f$ has period $2\pi$. Show that the Fourier series generated by $f$ assumes the following special forms under the conditions stated:
\begin{enumerate}[label=(\alph*)]
\item If $f(-x) = f(x)$ when $0 < x < \pi$, then
\[
f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty a_n \cos nx,
\]
where $a_n = \frac{2}{\pi} \int_0^\pi f(t) \cos nt \, dt$.
\item If $f(-x) = -f(x)$ when $0 < x < \pi$, then
\[
f(x) \sim \sum_{n=1}^\infty b_n \sin nx,
\]
where $b_n = \frac{2}{\pi} \int_0^\pi f(t) \sin nt \, dt$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the symmetry properties of even and odd functions to show that certain Fourier coefficients vanish, reducing the series to cosine or sine terms only.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item If $f$ is even, then $b_n=\tfrac{1}{\pi}\int_{-\pi}^{\pi} f(t)\sin nt\,dt=0$. Hence the series reduces to the cosine series with
\[a_n=\frac{1}{\pi}\int_{-\pi}^{\pi} f(t)\cos nt\,dt=\frac{2}{\pi}\int_0^{\pi} f(t)\cos nt\,dt.\]
\item If $f$ is odd, then $a_0=0$ and $a_n=0$ for $n\ge1$, leaving the sine series with
\[b_n=\frac{2}{\pi}\int_0^{\pi} f(t)\sin nt\,dt.\]
\end{enumerate}\qed


\begin{problembox}[11.9: Fourier Series for Linear and Quadratic Functions]
\begin{problemstatement}
Show that each of the expansions is valid in the range indicated.
\begin{enumerate}[label=(\alph*)]
\item $x = \pi - 2 \sum_{n=1}^\infty \frac{\sin nx}{n}$ if $0 < x < 2\pi$.
\begin{itemize}
\item \textit{Note:} When $x = 0$, this gives $\zeta(2) = \pi^2/6$.
\end{itemize}
\item $x^2 = \pi x - \frac{\pi^2}{3} + 2 \sum_{n=1}^\infty \frac{\cos nx}{n^2}$ if $0 < x < 2\pi$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Extend the functions to be $2\pi$-periodic and compute the Fourier coefficients directly using integration by parts.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item On $(0,2\pi)$ extend $f(x)=x$ to a $2\pi$-periodic sawtooth. Then $a_0=\tfrac{1}{\pi}\int_{-\pi}^{\pi} f=0$, $a_n=0$ by oddness about $x=\pi$, and
\[b_n=\frac{1}{\pi}\int_{-\pi}^{\pi} x\sin nx\,dx=\frac{2}{n}.\]
Thus $x\sim \pi-2\sum_{n=1}^{\infty} \dfrac{\sin nx}{n}$ for $0<x<2\pi$.
\item For $f(x)=x^2$ on $(0,2\pi)$ extended periodically, $b_n=0$ by evenness about $x=\pi$, and
\[a_0=\frac{1}{\pi}\int_{-\pi}^{\pi} x^2\,dx=\frac{2\pi^2}{3},\quad a_n=\frac{1}{\pi}\int_{-\pi}^{\pi} x^2\cos nx\,dx=\frac{4}{n^2}.\]
Hence $x^2\sim \pi x-\tfrac{\pi^2}{3}+2\sum_{n=1}^{\infty}\dfrac{\cos nx}{n^2}$ on $0<x<2\pi$.
\end{enumerate}\qed


\begin{problembox}[11.10: Fourier Series for Odd and Even Terms]
\begin{problemstatement}
Show that each of the expansions is valid in the range indicated.
\begin{enumerate}[label=(\alph*)]
\item $|x| = \frac{\pi}{2} - \frac{4}{\pi} \sum_{n=1}^\infty \frac{\sin (2n - 1)x}{2n - 1}$ if $0 < x < \pi$.
\item $x = \frac{4}{\pi} \sum_{n=1}^\infty \frac{\cos (2n - 1)x}{(2n - 1)^2}$ if $-\pi < x < \pi$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the even/odd properties of the functions to determine which coefficients vanish, and compute the remaining coefficients by direct integration.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item Apply the cosine series for the even function $|x|$ on $(-\pi,\pi)$, or equivalently subtract the even part of the series in 11.9(a). Only odd harmonics remain, yielding the stated expansion.
\item Apply the sine series for the odd function $x$ on $(-\pi,\pi)$ and observe only odd cosine terms appear after integrating termwise. Compute coefficients directly to obtain the given series.
\end{enumerate}\qed


\begin{problembox}[11.11: Fourier Series for Linear Functions]
\begin{problemstatement}
Show that each of the expansions is valid in the range indicated.
\begin{enumerate}[label=(\alph*)]
\item $x = 2 \sum_{n=1}^\infty \frac{(-1)^{n-1} \sin nx}{n}$ if $-\pi < x < \pi$.
\item $x^2 = \frac{\pi^2}{3} + 4 \sum_{n=1}^\infty \frac{(-1)^n \cos nx}{n^2}$ if $-\pi < x < \pi$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Similar to 11.9 but with domain $(-\pi,\pi)$, compute Fourier coefficients using integration by parts and note the alternating signs.

\bigskip\noindent\textbf{Solution:}
Identical to 11.9 with domain $(-\pi,\pi)$: for $f(x)=x$, one finds $b_n=2(-1)^{n-1}/n$ and $a_n=0$, giving (a). For $f(x)=x^2$ on $(-\pi,\pi)$, $a_0=2\pi^2/3$, $a_n=4(-1)^n/n^2$, $b_n=0$, giving (b).\qed


\begin{problembox}[11.12: Fourier Series for Trigonometric Functions]
\begin{problemstatement}
Show that each of the expansions is valid in the range indicated.
\begin{enumerate}[label=(\alph*)]
\item $\cos x = \frac{4}{\pi} - \frac{4}{\pi} \sum_{n=1}^\infty \frac{\cos (2n - 1)x}{(2n - 1)^2}$ if $-\pi < x < \pi$.
\item $\sin x = \frac{2}{\pi} - \frac{4}{\pi} \sum_{n=1}^\infty \frac{\cos 2nx}{4n^2 - 1}$ if $-\pi < x < \pi$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Expand the trigonometric functions in cosine series on $(-\pi,\pi)$, using the fact that only certain harmonics appear due to the functions' properties.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item Expand $\cos x$ in a cosine series on $(-\pi,\pi)$. Coefficients vanish for even indices; compute $a_0=\tfrac{4}{\pi}$ and $a_{2n-1}=-\tfrac{4}{\pi(2n-1)^2}$ to get the formula.
\item Expand $\sin x$ in a cosine series by integrating the series for the square wave of 11.19(a), or compute directly: only even modes appear with $a_{2n}= -\tfrac{4}{\pi(4n^2-1)}$.
\end{enumerate}\qed


\begin{problembox}[11.13: Fourier Series for Cosine and Sine]
\begin{problemstatement}
Show that each of the expansions is valid in the range indicated.
\begin{enumerate}[label=(\alph*)]
\item $\cos x = \frac{\pi}{2} - \frac{8}{\pi} \sum_{n=1}^\infty \frac{n \sin 2nx}{4n^2 - 1}$ if $0 < x < 2\pi$.
\item $\sin x = \frac{2}{\pi} - \frac{4}{\pi} \sum_{n=1}^\infty \frac{\cos 2nx}{4n^2 - 1}$ if $0 < x < \pi$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Differentiate the series from 11.12(b) termwise (justified by uniform convergence) and integrate as needed to obtain the stated identities.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item Differentiate the series in 11.12(b) termwise (justified by uniform convergence on compact subsets), then integrate as needed to obtain the stated identity for $\cos x$ on $(0,2\pi)$.
\item Same method starting from 11.12(b) and restricting to $(0,\pi)$ gives the stated sine expansion.
\end{enumerate}\qed


\begin{problembox}[11.14: Fourier Series for Products]
\begin{problemstatement}
Show that each of the expansions is valid in the range indicated.
\begin{enumerate}[label=(\alph*)]
\item $x \cos x = -\frac{1}{2} \sin x + 2 \sum_{n=2}^\infty \frac{(-1)^{n-1} n \sin nx}{n^2 - 1}$ if $-\pi < x < \pi$.
\item $x \sin x = \frac{1}{2} - \frac{1}{2} \cos x - 2 \sum_{n=2}^\infty \frac{(-1)^n \cos nx}{n^2 - 1}$ if $-\pi < x < \pi$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Multiply the series for $x$ by $\cos x$ and $\sin x$ respectively, using product-to-sum identities or compute coefficients directly using integration.

\bigskip\noindent\textbf{Solution:}
Multiply the series for $x$ (from 11.11(a)) by $\cos x$ and use product-to-sum identities, or compute coefficients directly: for $x\cos x$, $a_1= -\tfrac{1}{2}$, $b_n=\dfrac{2(-1)^{n-1}n}{n^2-1}$ for $n\ge2$ with $b_1=0$, yielding (a). For $x\sin x$, one finds $a_0=\tfrac{1}{2}$, $a_1=-\tfrac{1}{2}$, and $a_n= -\dfrac{2(-1)^n}{n^2-1}$ for $n\ge2$, giving (b).\qed


\begin{problembox}[11.15: Fourier Series for Logarithmic Functions]
\begin{problemstatement}
Show that each of the expansions is valid in the range indicated.
\begin{enumerate}[label=(\alph*)]
\item $\log \left| \sin \frac{x}{2} \right| = -\log 2 - \sum_{n=1}^\infty \frac{\cos nx}{n}$ if $x \neq 2k\pi$ (k an integer).
\item $\log \left| \cos \frac{x}{2} \right| = -\log 2 - \sum_{n=1}^\infty \frac{(-1)^n \cos nx}{n}$ if $x \neq (2k + 1)\pi$.
\item $\log \left| \tan \frac{x}{2} \right| = -2 \sum_{n=1}^\infty \frac{\cos (2n - 1)x}{2n - 1}$ if $x \neq (2k + 1)\pi$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Consider the even function $f(x)=-\log(2\sin|x|/2)$ on $(-\pi,\pi)$, compute its cosine coefficients, and use trigonometric identities to derive the other expansions.

\bigskip\noindent\textbf{Solution:}
Consider $f(x)= -\log\big(2\sin\tfrac{|x|}{2}\big)$ on $(-\pi,\pi)$, which is even and integrable. Compute its cosine coefficients using
\[\int_0^{\pi} \log(\sin(t/2))\,\cos nt\,dt = -\frac{\pi}{2n}\quad (n\ge1),\]
to obtain (a). Replace $x$ by $\pi-x$ to get (b). Subtract (b) from (a) to derive (c).\qed


\begin{problembox}[11.16: Fourier Series and Zeta Function]
\begin{problemstatement}
\begin{enumerate}[label=(\alph*)]
\item Find a continuous function on $[-\pi, \pi]$ which generates the Fourier series $\sum_{n=1}^\infty \frac{(-1)^{n-1}}{n^3} \sin nx$. Then use Parseval's formula to prove that $\zeta(6) = \frac{\pi^6}{945}$.
\item Use an appropriate Fourier series in conjunction with Parseval's formula to show that $\zeta(4) = \frac{\pi^4}{90}$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For part (a), construct a cubic polynomial that generates the given sine coefficients. For part (b), apply Parseval's formula to the cosine series of $x^2$ from previous exercises.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item Take $f(x)=\tfrac{\pi^2}{12}x-\tfrac{\pi}{4}x^2+\tfrac{1}{12}x^3$ on $[-\pi,\pi]$ made periodic; then $b_n=\dfrac{(-1)^{n-1}}{n^3}$. Parseval gives
\[\sum_{n=1}^{\infty}\frac{1}{n^6}=\frac{\pi^6}{945}.\]
\item Apply Parseval to the cosine series of $x^2$ in 11.11(b):
\[\frac{1}{\pi}\int_{-\pi}^{\pi}\big(x^2-\tfrac{\pi^2}{3}\big)^2 dx = 2\sum_{n=1}^{\infty}\frac{1}{n^4}\Rightarrow \zeta(4)=\frac{\pi^4}{90}.\]
\end{enumerate}\qed


\begin{problembox}[11.17: Parseval's Formula Application]
\begin{problemstatement}
Assume that $f$ has a continuous derivative on $[0, 2\pi]$, that $f(0) = f(2\pi)$, and that $\int_0^{2\pi} f(t) \, dt = 0$. Prove that
\[
\|f'\| \geq \|f\|,
\]
with equality if and only if $f(x) = a \cos x + b \sin x$.
\begin{itemize}
\item \textit{Hint:} Use Parseval's formula.
\end{itemize}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use Parseval's formula to express $\|f\|^2$ and $\|f'\|^2$ in terms of Fourier coefficients, noting that the derivative multiplies coefficients by $n$, and apply the condition that $a_0=0$.

\bigskip\noindent\textbf{Solution:}
Let $f(x)\sim \tfrac{a_0}{2}+\sum_{n\ge1}(a_n\cos nx+b_n\sin nx)$. Since $\int_0^{2\pi}f=0$, we have $a_0=0$. Then $\|f\|^2=\pi\sum_{n\ge1}(a_n^2+b_n^2)$ and $\|f'\|^2=\pi\sum_{n\ge1} n^2(a_n^2+b_n^2)\ge\|f\|^2$, with equality iff all terms vanish unless $n=1$, i.e., $f(x)=a\cos x+b\sin x$.\qed


\begin{problembox}[11.18: Bernoulli Functions]
\begin{problemstatement}
A sequence $\{B_n\}$ of periodic functions (of period 1) is defined on $\mathbb{R}$ as follows:
\[
B_{2n}(x) = (-1)^{n+1} \frac{2}{(2n)!} \sum_{k=1}^\infty \frac{\cos 2\pi k x}{(\pi k)^{2n}}, \quad (n = 0, 1, 2, \dots),
\]
\[
B_{2n+1}(x) = \frac{2}{(2n + 1)!} \sum_{k=1}^\infty \frac{\sin 2\pi k x}{(\pi k)^{2n+1}}, \quad (n = 0, 1, 2, \dots).
\]
($B_n$ is called the Bernoulli function of order $n$.) Show that:
\begin{enumerate}[label=(\alph*)]
\item $B_1(x) = x - [x] - \frac{1}{2}$ if $x$ is not an integer. ($[x]$ is the greatest integer $\leq x$.)
\item $\int_0^1 B_n(x) \, dx = 0$ if $n \geq 1$ and $B_n'(x) = n B_{n-1}(x)$ if $n \geq 2$.
\item $B_n(x) = P_n(x)$ if $0 < x < 1$, where $P_n$ is the $n$th Bernoulli polynomial. (See Exercise 9.38 for the definition of $P_n$.)
\item $B_n(x) = -\sum_{k \neq 0} \frac{e^{2\pi i k x}}{(2\pi i k)^n}$, ($n = 1, 2, \dots$).
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Compute the Fourier series of the sawtooth function for part (a), integrate and differentiate the series termwise for part (b), compare with known Bernoulli polynomial expansions for part (c), and combine sine/cosine series into exponentials for part (d).

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item Compute the Fourier series of $x-[x]-\tfrac12$ on $[0,1]$ to match the given sine series.
\item Integrate termwise to see $\int_0^1 B_n=0$ for $n\ge1$; differentiate series to get $B_n'=nB_{n-1}$ for $n\ge2$.
\item Compare with the standard Fourier expansions of Bernoulli polynomials $P_n$ on $(0,1)$.
\item Combine the sine/cosine series into exponentials to obtain $B_n(x)=-\sum_{k\ne0}\dfrac{e^{2\pi i k x}}{(2\pi i k)^n}$.
\end{enumerate}\qed


\begin{problembox}[11.19: Gibbs' Phenomenon]
\begin{problemstatement}
Let $f$ be the function of period $2\pi$ whose values on $[-\pi, \pi]$ are
\[
f(x) = 
\begin{cases} 
1 & \text{if } 0 < x < \pi, \\
0 & \text{if } x = 0 \text{ or } x = \pi, \\
-1 & \text{if } -\pi < x < 0.
\end{cases}
\]
\begin{enumerate}[label=(\alph*)]
\item Show that
\[
f(x) = \frac{4}{\pi} \sum_{n=1}^\infty \frac{\sin (2n - 1)x}{2n - 1},
\]
for every $x$.
\item Show that
\[
s_n(x) = \frac{2}{\pi} \int_0^x \frac{\sin 2nt}{\sin t} \, dt,
\]
where $s_n(x)$ denotes the $n$th partial sum of the series in part (a).
\item Show that, in $(0, \pi)$, $s_n$ has local maxima at $x_1, x_3, \dots, x_{2n-1}$ and local minima at $x_2, x_4, \dots, x_{2n-2}$, where $x_m = \frac{m\pi}{n}$ ($m = 1, 2, \dots, 2n - 1$).
\item Show that $s_n\left(\frac{\pi}{n}\right)$ is the largest of the numbers $s_n(x_m)$ ($m = 1, 2, \dots, 2n - 1$).
\item Interpret $s_n\left(\frac{\pi}{n}\right)$ as a Riemann sum and prove that
\[
\lim_{n \to \infty} s_n\left(\frac{\pi}{n}\right) = \frac{2}{\pi} \int_0^\pi \frac{\sin t}{t} \, dt.
\]
The value of the limit in (e) is about 1.179. Thus, although $f$ has a jump equal to 2 at the origin, the graphs of the approximating curves $s_n$ tend to approximate a vertical segment of length 2.358 in the vicinity of the origin. This is the Gibbs phenomenon.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Compute sine coefficients for the odd square wave, identify the Dirichlet kernel in partial sums, locate critical points using zeros of $\sin 2nt$, compare values at critical points, and interpret the limit as Riemann sums for the sinc integral.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item Compute the sine coefficients for the odd square wave: $b_{2n}=0$, $b_{2n-1}=\tfrac{4}{\pi(2n-1)}$.
\item Differentiate the partial sum to identify the Dirichlet kernel and integrate to obtain $s_n(x)=\tfrac{2}{\pi}\int_0^x \dfrac{\sin 2nt}{\sin t}\,dt$.
\item Differentiate $s_n$ and locate critical points using zeros of $\sin 2nt$.
\item Compare values at the critical points using monotonicity between them.
\item Interpret as Riemann sums for $\int_0^{\pi} \dfrac{\sin t}{t}\,dt$ to get the limit.
\end{enumerate}\qed


\begin{problembox}[11.20: Fourier Coefficients of Bounded Variation]
\begin{problemstatement}
If $f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty (a_n \cos nx + b_n \sin nx)$ and if $f$ is of bounded variation on $[0, 2\pi]$, show that $a_n = O(1/n)$ and $b_n = O(1/n)$.
\begin{itemize}
\item \textit{Hint:} Write $f = g - h$, where $g$ and $h$ are increasing on $[0, 2\pi]$. Then
\[
a_n = \frac{2}{n\pi} \int_0^{2\pi} g(x) \, d(\sin nx) - \frac{2}{n\pi} \int_0^{2\pi} h(x) \, d(\sin nx).
\]
Now apply Theorem 7.31.
\end{itemize}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Decompose $f$ as the difference of two increasing functions, integrate by parts in the Riemann-Stieltjes sense, and apply Theorem 7.31 to bound the resulting integrals.

\bigskip\noindent\textbf{Solution:}
Write $f=g-h$ with $g,h$ increasing. Integrate by parts in the Riemann–Stieltjes sense:
\[a_n=\frac{2}{\pi n}\int_0^{2\pi} g\,d(\sin nx)-\frac{2}{\pi n}\int_0^{2\pi} h\,d(\sin nx).\]
By Theorem 7.31, the integrals are $O(1)$, hence $a_n=O(1/n)$. Similarly for $b_n$.\qed


\begin{problembox}[11.21: Lipschitz Condition and Lebesgue Integral]
\begin{problemstatement}
Suppose $g \in L([a, \delta])$ for every $a$ in $(0, \delta)$ and assume that $g$ satisfies a "right-handed" Lipschitz condition at 0. (See the Note following Theorem 11.9.) Show that the Lebesgue integral $\int_0^\delta \frac{|g(t) - g(0+)|}{t} \, dt$ exists.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the Lipschitz condition to bound the integrand near 0, showing it is bounded and hence integrable on the entire interval.

\bigskip\noindent\textbf{Solution:}
If $|g(t)-g(0+)|\le C t$ for small $t$ (right Lipschitz), then $\dfrac{|g(t)-g(0+)|}{t}\le C$ near $0$, and the function is integrable on $(0,\delta)$ since it is bounded near $0$ and integrable away from $0$ by hypothesis. Hence the Lebesgue integral exists.\qed


\begin{problembox}[11.22: Fourier Series Convergence]
\begin{problemstatement}
Use Exercise 11.21 to prove that differentiability of $f$ at a point implies convergence of its Fourier series at the point.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Show that differentiability implies a Lipschitz condition on the function $g(t)=f(x_0+t)+f(x_0-t)-2f(x_0)$, then apply 11.21 to justify the Dirichlet-Jordan test.

\bigskip\noindent\textbf{Solution:}
If $f$ is differentiable at $x_0$, then $g(t)=f(x_0+t)+f(x_0-t)-2f(x_0)=o(1)$ as $t\to0^+$, and $g$ satisfies a right Lipschitz condition at $0$. Apply 11.21 to justify the Dirichlet–Jordan test at $x_0$, which gives convergence of the Fourier series to $f(x_0)$.\qed


\begin{problembox}[11.23: Orthogonality to Polynomials]
\begin{problemstatement}
Let $g$ be continuous on $[0, 1]$ and assume that $\int_0^1 t^n g(t) \, dt = 0$ for $n = 0, 1, 2, \dots$. Show that:
\begin{enumerate}[label=(\alph*)]
\item $\int_0^1 g(t)^2 \, dt = \int_0^1 g(t)(g(t) - P(t)) \, dt$ for every polynomial $P$.
\item $\int_0^1 g(t)^2 \, dt = 0$.
\begin{itemize}
\item \textit{Hint:} Use Theorem 11.17.
\end{itemize}
\item $g(t) = 0$ for every $t$ in $[0, 1]$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the orthogonality condition to show that $\int_0^1 g^2 = \int_0^1 g(g-P)$ for any polynomial $P$, then use Weierstrass approximation to choose $P$ close to $g$, and finally use continuity to conclude $g\equiv0$.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item For any polynomial $P$, $\int_0^1 g(g-P)=\int_0^1 g^2-\int_0^1 gP=\int_0^1 g^2$, since $\int_0^1 t^n g(t)\,dt=0$ for each monomial in $P$.
\item Choosing $P=g$ in (a) and approximating $g$ uniformly by polynomials on $[0,1]$ (Weierstrass) yields $\int_0^1 g^2=0$.
\item Since $g$ is continuous and nonnegative $\int_0^1 g^2=0$ implies $g\equiv0$ on $[0,1]$.
\end{enumerate}\qed


\begin{problembox}[11.24: Weierstrass Approximation]
\begin{problemstatement}
Use the Weierstrass approximation theorem to prove each of the following statements.
\begin{enumerate}[label=(\alph*)]
\item If $f$ is continuous on $[1, +\infty)$ and if $f(x) \to a$ as $x \to +\infty$, then $f$ can be uniformly approximated on $[1, +\infty)$ by a function $g$ of the form $g(x) = p(1/x)$, where $p$ is a polynomial.
\item If $f$ is continuous on $[0, +\infty)$ and if $f(x) \to a$ as $x \to +\infty$, then $f$ can be uniformly approximated on $[0, +\infty)$ by a function $g$ of the form $g(x) = p(e^{-x})$, where $p$ is a polynomial.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use change of variables to map the infinite intervals to compact intervals, apply Weierstrass approximation on the compact intervals, then transform back to the original domains.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item Map $x\mapsto t=1/x$ from $[1,\infty)$ to $(0,1]$ and approximate $h(t)=f(1/t)$ uniformly on $[0,1]$ by polynomials $p(t)$. Then $g(x)=p(1/x)$ approximates $f$ uniformly on $[1,\infty)$.
\item Map $x\mapsto t=e^{-x}$ from $[0,\infty)$ to $(0,1]$ and approximate $h(t)=f(-\log t)$ uniformly on $[0,1]$ by polynomials $p(t)$. Then $g(x)=p(e^{-x})$ approximates $f$ uniformly on $[0,\infty)$.
\end{enumerate}\qed


\begin{problembox}[11.25: Arithmetic Means of Fourier Series]
\begin{problemstatement}
Assume that $f(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty (a_n \cos nx + b_n \sin nx)$ and let $\{\sigma_n\}$ be the sequence of arithmetic means of the partial sums of this series, as it was given in (23). Show that:
\begin{enumerate}[label=(\alph*)]
\item $\sigma_n(x) = \frac{a_0}{2} + \sum_{k=1}^{n-1} \left(1 - \frac{k}{n}\right) (a_k \cos kx + b_k \sin kx)$.
\item $\int_0^{2\pi} |f(x) - \sigma_n(x)|^2 \, dx = \frac{\pi}{n^2} \sum_{k=1}^{n-1} k^2 (a_k^2 + b_k^2)$.
\item If $f$ is continuous on $[0, 2\pi]$ and has period $2\pi$, then
\[
\lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^{n-1} k^2 (a_k^2 + b_k^2) = 0.
\]
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Express the Cesàro means as weighted sums of Fourier coefficients, apply Parseval's formula to compute the $L^2$ error, and use Fejér's theorem on uniform convergence for continuous functions.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item The $n$th Cesàro mean is $\sigma_n=\tfrac{1}{n}\sum_{m=0}^{n-1} s_m$, where $s_m$ is the $m$th partial sum. Summing coefficients gives the listed weights $1-\tfrac{k}{n}$.
\item Apply Parseval to $f-\sigma_n$ using orthogonality to obtain the integral identity.
\item If $f$ is continuous and periodic, Fejér's theorem gives $\sigma_n\to f$ uniformly, hence the weighted sum tends to $0$.
\end{enumerate}\qed


\begin{problembox}[11.26: Convergence of Exponential Fourier Series]
\begin{problemstatement}
Consider the Fourier series (in exponential form) generated by a function $f$ which is continuous on $[0, 2\pi]$ and periodic with period $2\pi$, say
\[
f(x) \sim \sum_{n=-\infty}^{+\infty} a_n e^{inx}.
\]
Assume also that the derivative $f' \in \mathcal{R}$ on $[0, 2\pi]$. 
\begin{enumerate}[label=(\alph*)]
\item Prove that the series $\sum n^2 |a_n|^2$ converges; then use the Cauchy-Schwarz inequality to deduce that $\sum |a_n|$ converges.
\item From (a), deduce that the series $\sum a_n e^{inx}$ converges uniformly to a continuous sum function $g$ on $[0, 2\pi]$. Then prove that $f = g$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use Parseval's formula for $f'$ to show $\sum n^2|a_n|^2<\infty$, apply Cauchy-Schwarz with $\sum n^{-2}<\infty$ to get absolute convergence, then use uniqueness of Fourier coefficients to show $f=g$.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item If $f'\in\mathcal R$, then Parseval on $f'$ yields $\sum n^2|a_n|^2<\infty$. By Cauchy–Schwarz, $\sum |a_n|\le\big(\sum n^2|a_n|^2\big)^{1/2}\big(\sum n^{-2}\big)^{1/2}<\infty$.
\item Absolute convergence implies uniform convergence to a continuous $g$. The Fourier coefficients of $g$ equal $a_n$, which are those of $f$, so $f-g$ has all zero coefficients. By uniqueness (11.6), $f=g$.
\end{enumerate}\qed
\section{Fourier Integrals}

\subsection*{Essential Definitions and Theorems}

\begin{definition}[Fourier Transform]
For a function $f \in L^1(\mathbb{R})$, the Fourier transform is defined as:
\[\hat{f}(\xi) = \int_{-\infty}^{\infty} f(x) e^{-2\pi i \xi x} \, dx\]
The inverse Fourier transform is:
\[f(x) = \int_{-\infty}^{\infty} \hat{f}(\xi) e^{2\pi i \xi x} \, d\xi\]
\end{definition}

\noindent\textbf{Importance:} The Fourier transform extends the idea of Fourier series to non-periodic functions. It decomposes functions into their frequency components and is one of the most important tools in modern analysis. The transform converts convolution to multiplication, making it essential for solving differential equations and signal processing.



\begin{definition}[Cosine and Sine Transforms]
For an even function $f$, the cosine transform is:
\[\hat{f}_c(\xi) = 2 \int_0^{\infty} f(x) \cos(2\pi \xi x) \, dx\]
For an odd function $f$, the sine transform is:
\[\hat{f}_s(\xi) = 2 \int_0^{\infty} f(x) \sin(2\pi \xi x) \, dx\]
\end{definition}

\noindent\textbf{Importance:} These transforms are natural for functions with symmetry properties. They simplify calculations and are often more appropriate for physical problems with natural boundary conditions. The cosine transform is particularly useful for problems with Neumann boundary conditions.



\begin{theorem}[Fourier Inversion Theorem]
If $f \in L^1(\mathbb{R})$ and $\hat{f} \in L^1(\mathbb{R})$, then:
\[f(x) = \int_{-\infty}^{\infty} \hat{f}(\xi) e^{2\pi i \xi x} \, d\xi\]
almost everywhere. If $f$ is continuous, the equality holds everywhere.
\end{theorem}

\noindent\textbf{Importance:} This theorem shows that the Fourier transform is invertible, meaning we can recover the original function from its transform. This is crucial for applications where we need to reconstruct signals or solve inverse problems. The condition that both $f$ and $\hat{f}$ be integrable is important.



\begin{theorem}[Plancherel's Theorem]
The Fourier transform extends to an isometry on $L^2(\mathbb{R})$. That is, for $f, g \in L^2(\mathbb{R})$:
\[\int_{-\infty}^{\infty} f(x) \overline{g(x)} \, dx = \int_{-\infty}^{\infty} \hat{f}(\xi) \overline{\hat{g}(\xi)} \, d\xi\]
In particular, $\|f\|_2 = \|\hat{f}\|_2$.
\end{theorem}

\noindent\textbf{Importance:} This theorem shows that the Fourier transform preserves the $L^2$ inner product and norm. This is crucial for applications in quantum mechanics (where it's related to the uncertainty principle) and signal processing (where it preserves energy). It also provides a way to define the Fourier transform for $L^2$ functions.





\begin{problembox}[11.27: Fourier Integral for Even and Odd Functions]
\begin{problemstatement}
If $f$ satisfies the hypotheses of the Fourier integral theorem, show that:
\begin{enumerate}[label=(\alph*)]
\item If $f$ is even, that is, if $f(-t) = f(t)$ for every $t$, then
\[
\frac{f(x+) + f(x-)}{2} = \frac{2}{\pi} \int_0^\infty \left[ \int_0^\infty f(u) \cos vu \, du \right] \cos vx \, dv.
\]
\item If $f$ is odd, that is, if $f(-t) = -f(t)$ for every $t$, then
\[
\frac{f(x+) + f(x-)}{2} = \frac{2}{\pi} \int_0^\infty \left[ \int_0^\infty f(u) \sin vu \, du \right] \sin vx \, dv.
\]
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the Fourier integral theorem to $f$ and use the even/odd symmetry to reduce the two-sided transform to cosine or sine transforms.

\bigskip\noindent\textbf{Solution:}
Apply the Fourier integral theorem to $f$ and use even/odd symmetry to reduce the two-sided transform to cosine or sine transforms, yielding the stated formulas.\qed


\begin{problembox}[11.28: Fourier Integral Evaluation]
\begin{problemstatement}
Use the Fourier integral theorem to evaluate the improper integral:
\[
\int_0^\infty \frac{\sin v \cos vx}{v} \, dv = 
\begin{cases} 
\frac{\pi}{2} & \text{if } -1 < x < 1, \\
0 & \text{if } |x| > 1, \\
\frac{\pi}{4} & \text{if } |x| = 1.
\end{cases}
\]
\begin{itemize}
\item \textit{Suggestion:} Use Exercise 11.27 when possible.
\end{itemize}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Consider the even function $f(u)=\sin u/u$ and apply the cosine transform from 11.27 to identify this integral as the Fourier transform of a characteristic function.

\bigskip\noindent\textbf{Solution:}
Consider $f(u)=\dfrac{\sin u}{u}$, which is even and satisfies the hypotheses. By 11.27,
\[\int_0^{\infty}\frac{\sin v\cos vx}{v}\,dv=\frac{\pi}{2}\,\chi_{(-1,1)}(x)+\frac{\pi}{4}\,\chi_{\{|x|=1\}}(x).\]\qed


\begin{problembox}[11.29: Fourier Integral with Exponential]
\begin{problemstatement}
Use the Fourier integral theorem to evaluate the improper integral:
\[
\int_0^\infty \cos ax e^{-b|x|} \, dx = \frac{2b}{b^2 + a^2}, \quad \text{if } b > 0.
\]
\begin{itemize}
\item \textit{Hint:} Apply Exercise 11.27 with $f(u) = e^{-b|u|}$.
\end{itemize}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Let $f(u)=e^{-b|u|}$ (which is even) and compute its cosine transform directly, then use the Fourier integral theorem to evaluate the given integral.

\bigskip\noindent\textbf{Solution:}
Let $f(u)=e^{-b|u|}$, which is even. Its cosine transform equals $\dfrac{2b}{b^2+a^2}$, giving the stated value of $\int_0^{\infty}e^{-b|x|}\cos ax\,dx$ for $b>0$.\qed


\begin{problembox}[11.30: Fourier Integral with Rational Function]
\begin{problemstatement}
Use the Fourier integral theorem to evaluate the improper integral:
\[
\int_0^\infty \frac{x \sin ax}{1 + x^2} \, dx = \pi e^{-|a|}, \quad \text{if } a \neq 0.
\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Let $f(x)=x/(1+x^2)$ (an odd function) and compute its sine transform, or use the fact that this is related to the derivative of the Fourier transform of $e^{-|x|}$.

\bigskip\noindent\textbf{Solution:}
Let $f(x)=\dfrac{x}{1+x^2}$, an odd function. Its sine transform is $\int_0^{\infty}\dfrac{x\sin ax}{1+x^2}\,dx$, which equals $\pi e^{-|a|}$ by evaluating the Fourier transform of $e^{-|x|}$ and differentiating with respect to $a$.\qed


\begin{problembox}[11.31: Gamma Function Properties]
\begin{problemstatement}
\begin{enumerate}[label=(\alph*)]
\item Prove that
\[
\Gamma(p) \Gamma(p) = \frac{2}{\Gamma(2p)} \int_0^1 x^{p-1} (1 - x)^{p-1} \, dx.
\]
\item Make a suitable change of variable in (a) and derive the duplication formula for the Gamma function:
\[
\Gamma(2p) \Gamma\left(\frac{1}{2}\right) = 2^{2p-1} \Gamma(p) \Gamma\left(p + \frac{1}{2}\right).
\]
\begin{itemize}
\item \textit{Note:} In Exercise 10.30, it is shown that $\Gamma\left(\frac{1}{2}\right) = \sqrt{\pi}$.
\end{itemize}
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Start from the definition of $\Gamma(p)$ and use a change of variables to express $\Gamma(p)^2$ as a double integral, then convert to polar coordinates to obtain the beta integral. For part (b), use the substitution $x=(1+z)/2$ to relate to the beta function.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item Start from $\Gamma(p)=\int_0^{\infty} t^{p-1}e^{-t}\,dt$ and write 
$$\Gamma(p)^2=\int_0^{\infty}\int_0^{\infty} x^{p-1}y^{p-1}e^{-(x+y)}\,dx\,dy.$$ Substitute $x=ru$, $y=r(1-u)$ to obtain the beta integral and the stated identity.
\item Substitute $x=\tfrac{1+z}{2}$ in (a) to get $B(p,\tfrac12)=\dfrac{\Gamma(p)\Gamma(1/2)}{\Gamma(p+1/2)}=2^{1-2p}\dfrac{\Gamma(p)^2}{\Gamma(2p)}$ and rearrange to the duplication formula.
\end{enumerate}\qed


\begin{problembox}[11.32: Fourier Transform of Gaussian]
\begin{problemstatement}
If $f(x) = e^{-x^2/2}$ and $g(x) = x f(x)$ for all $x$, prove that
\[
\int_0^\infty f(x) \cos xy \, dx = f(y), \quad \text{and} \quad \int_0^\infty g(x) \sin xy \, dx = g(y).
\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Show that the cosine transform $F(y)$ of $f$ satisfies a differential equation $F''+F=0$ with appropriate boundary conditions, then use uniqueness to show $F(y)=f(y)$. Apply similar reasoning for the sine transform of $g$.

\bigskip\noindent\textbf{Solution:}
Integrate by parts or differentiate under the integral sign noting $f'= -x f$ and $g'= f- x g$ to obtain that $F(y)=\int_0^{\infty} f(x)\cos(xy)\,dx$ satisfies $F''+F=0$ with $F(0)=\int_0^{\infty} e^{-x^2/2}\,dx=\sqrt{\pi/2}$ and $F'(0)=0$. Comparing with $f(y)=e^{-y^2/2}$ and using uniqueness gives $F(y)=f(y)$; similarly for $g$ with sine.\qed


\begin{problembox}[11.33: Poisson Summation Formula]
\begin{problemstatement}
This exercise describes another form of Poisson's summation formula. Assume that $f$ is nonnegative, decreasing, and continuous on $[0, +\infty)$ and that $\int_0^\infty f(x) \, dx$ exists as an improper Riemann integral. Let
\[
g(y) = \frac{2}{\pi} \int_0^\infty f(x) \cos xy \, dx.
\]
If $a$ and $b$ are positive numbers such that $ab = 2\pi$, prove that
\[
\sqrt{a} \left\{ f(0) + \sum_{m=1}^\infty f(ma) \right\} = \sqrt{b} \left\{ g(0) + \sum_{n=1}^\infty g(nb) \right\}.
\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the Poisson summation formula to the even extension of $f$ and its cosine transform $g$, using the hypotheses to ensure absolute convergence and justify termwise operations.

\bigskip\noindent\textbf{Solution:}
Apply the Poisson summation formula to the even extension of $f$ and its cosine transform $g$; the hypotheses ensure absolute convergence and justification of termwise operations. With $ab=2\pi$, rescale to obtain the stated equality.\qed


\begin{problembox}[11.34: Transformation Formula]
\begin{problemstatement}
Prove that the transformation formula (55) for $\theta(x)$ can be put in the form
\[
\sum_{m=1}^\infty e^{-\pi m^2 a^2} + \frac{1}{2} = \frac{1}{\sqrt{a}} \left( \sum_{n=1}^\infty e^{-\pi n^2 b^2} + \frac{1}{2} \right),
\]
where $ab = 2\pi$, $a > 0$.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply 11.33 to $f(x)=e^{-\pi a^2 x^2}$ to connect the two Gaussian sums, using $ab=2\pi$ and separating the $m=n=0$ terms.

\bigskip\noindent\textbf{Solution:}
Apply 11.33 to $f(x)=e^{-\pi a^2 x^2}$ to connect the two Gaussian sums. Using $ab=2\pi$ and separating the $m=n=0$ terms yields the displayed relation for $\theta$.\qed


\begin{problembox}[11.35: Zeta Function and Integral]
\begin{problemstatement}
If $s > 1$, prove that
\[
\pi^{-s/2} \sum_{n=1}^\infty n^{-s} = \frac{1}{\Gamma(s/2)} \int_0^\infty e^{-\pi x^2} x^{s/2-1} \, dx,
\]
and derive the formula
\[
\sum_{n=1}^\infty n^{-s} = \pi^{s/2} \Gamma\left(\frac{s}{2}\right) \zeta(s) = \int_0^\infty \left( \theta(x) - 1 \right) x^{s/2-1} \, dx,
\]
where $\theta(x) = \sum_{n=-\infty}^\infty e^{-\pi n^2 x}$. Use this and the transformation formula for $\theta(x)$ to prove that
\[
\pi^{s/2} \Gamma\left(\frac{s}{2}\right) \zeta(s) = \int_0^\infty \left[ x^{s/2-1} + x^{(1-s)/2-1} \right] \theta(x) \, dx.
\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the Mellin transform of $e^{-\pi x^2}$ to establish the first identity, then express the zeta function in terms of the theta function, and apply the transformation law for $\theta$ to obtain the final integral identity.

\bigskip\noindent\textbf{Solution:}
For $s>1$, use the Mellin transform of $e^{-\pi x^2}$:
\[\int_0^{\infty} e^{-\pi x^2} x^{s/2-1}\,dx = \tfrac{1}{2}\,\pi^{-s/4}\,\Gamma\!\left(\tfrac{s}{2}\right)\sum_{n=1}^{\infty} n^{-s},\]
which gives the first identity. Then
\[\sum_{n=1}^{\infty} n^{-s}=\pi^{s/2}\,\Gamma\!\left(\tfrac{s}{2}\right)\zeta(s)=\int_0^{\infty} (\theta(x)-1)\,x^{s/2-1}\,dx,\]
and applying the transformation law for $\theta$ yields the final integral identity.\qed
\section{Laplace Transforms}

\subsection*{Essential Definitions and Theorems}

\begin{definition}[Laplace Transform]
For a function $f$ defined on $[0, \infty)$, the Laplace transform is defined as:
\[\mathcal{L}\{f\}(s) = F(s) = \int_0^{\infty} e^{-st} f(t) \, dt\]
where $s$ is a complex variable with $\text{Re}(s) > \alpha$ for some $\alpha$ (the abscissa of convergence).
\end{definition}

\noindent\textbf{Importance:} The Laplace transform converts differential equations into algebraic equations, making it a powerful tool for solving initial value problems.

\begin{definition}[Inverse Laplace Transform]
The inverse Laplace transform recovers the original function from its transform:
\[f(t) = \mathcal{L}^{-1}\{F\}(t) = \frac{1}{2\pi i} \int_{c-i\infty}^{c+i\infty} e^{st} F(s) \, ds\]
where $c$ is chosen so that all singularities of $F(s)$ lie to the left of the line $\text{Re}(s) = c$.
\end{definition}

\noindent\textbf{Importance:} The inverse transform allows us to recover the solution to the original problem from the transformed equation.

\begin{theorem}[Properties of Laplace Transform]
\begin{enumerate}[label=(\alph*)]
\item Linearity: $\mathcal{L}\{af + bg\} = a\mathcal{L}\{f\} + b\mathcal{L}\{g\}$
\item First derivative: $\mathcal{L}\{f'\}(s) = sF(s) - f(0)$
\item Second derivative: $\mathcal{L}\{f''\}(s) = s^2F(s) - sf(0) - f'(0)$
\item Integration: $\mathcal{L}\{\int_0^t f(\tau) \, d\tau\}(s) = \frac{F(s)}{s}$
\item Convolution: $\mathcal{L}\{f * g\}(s) = F(s)G(s)$
\end{enumerate}
\end{theorem}

\noindent\textbf{Importance:} These properties are what make the Laplace transform so useful for solving differential equations. The derivative properties convert differential operators into algebraic operations, while the convolution property is essential for solving integral equations and systems with memory effects.



\begin{theorem}[Final Value Theorem]
If $\lim_{t \to \infty} f(t)$ exists and $F(s) = \mathcal{L}\{f\}(s)$, then:
\[\lim_{t \to \infty} f(t) = \lim_{s \to 0} sF(s)\]
\end{theorem}

\noindent\textbf{Importance:} This theorem allows us to determine the long-term behavior of a system without computing the full inverse transform. It's particularly useful in control theory for determining steady-state responses and in engineering for analyzing system stability.





\begin{problembox}[11.36: Laplace Transform Table]
\begin{problemstatement}
Verify the entries in the following table of Laplace transforms:
\[
\begin{array}{ll}
f(t) & F(z) = \int_0^\infty e^{-zt} f(t) \, dt, \quad z = x + iy \\
e^{at} & (z - a)^{-1}, \quad (x > a) \\
\cos at & z/(z^2 + a^2), \quad (x > 0) \\
\sin at & a/(z^2 + a^2), \quad (x > 0) \\
t^p e^{at} & \Gamma(p + 1)/(z - a)^{p+1}, \quad (x > a, p > 0)
\end{array}
\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Compute the Laplace transforms directly from the definition $F(z)=\int_0^\infty e^{-zt}f(t)\,dt$, using integration by parts and the properties of the Gamma function.

\bigskip\noindent\textbf{Solution:}
Compute directly from $F(z)=\int_0^{\infty} e^{-zt} f(t)\,dt$: for $f(t)=e^{at}$, $F(z)=(z-a)^{-1}$ when $\Re z>a$. Using Euler's formulas for $\cos at$ and $\sin at$ gives the listed entries. For $t^p e^{at}$, integrate by parts or use $\Gamma$ to obtain $\Gamma(p+1)/(z-a)^{p+1}$.\qed


\begin{problembox}[11.37: Convolution and Laplace Transform]
\begin{problemstatement}
Show that the convolution $h = f * g$ assumes the form
\[
h(t) = \int_0^t f(x) g(t - x) \, dx,
\]
when both $f$ and $g$ vanish on the negative real axis. Use the convolution theorem for Fourier transforms to prove that $\mathcal{L}(f * g) = \mathcal{L}(f) \cdot \mathcal{L}(g)$.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the fact that $f$ and $g$ vanish on $(-\infty,0)$ to reduce the convolution integral to $[0,t]$, then apply the Fourier convolution theorem and restrict to $t>0$ to obtain the Laplace transform result.

\bigskip\noindent\textbf{Solution:}
If $f$ and $g$ vanish on $(-\infty,0)$, then $(f*g)(t)=\int_{-\infty}^{\infty} f(x)g(t-x)\,dx=\int_0^t f(x)g(t-x)\,dx$. The Fourier transform converts convolution into multiplication; restricting to $t>0$ gives $\mathcal L(f*g)=\mathcal L(f)\,\mathcal L(g)$.\qed


\begin{problembox}[11.38: Properties of Laplace Transform]
\begin{problemstatement}
Assume $f$ is continuous on $(0, +\infty)$ and let $F(z) = \int_0^\infty e^{-zt} f(t) \, dt$ for $z = x + iy$, $x > c > 0$. If $s > c$ and $a > 0$, prove that:
\begin{enumerate}[label=(\alph*)]
\item $F(s + a) = a \int_0^\infty g(t) e^{-at} \, dt$, where $g(x) = \int_0^\infty e^{-st} f(t) \, dt$.
\item If $F(s + na) = 0$ for $n = 0, 1, 2, \dots$, then $f(t) = 0$ for $t > 0$.
\begin{itemize}
\item \textit{Hint:} Use Exercise 11.23.
\end{itemize}
\item If $h$ is continuous on $(0, +\infty)$ and if $f$ and $h$ have the same Laplace transform, then $f(t) = h(t)$ for every $t > 0$.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For part (a), use a change of variables in the double integral. For part (b), use 11.23 applied to $g$ on $[0,\infty)$ via compactification. For part (c), use the uniqueness of inverse Laplace transforms.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item With $g(x)=\int_0^{\infty} e^{-st} f(t)\,dt$ and $a>0$,
\begin{align*} 
F(s+a)&=\int_0^{\infty} e^{-(s+a)t}f(t)\,dt=a\int_0^{\infty} \Big(\int_t^{\infty} e^{-au}\,du\Big) e^{-st}f(t)\,dt\\
&=a\int_0^{\infty} g(t)e^{-at}\,dt.
\end{align*}
\item If $F(s+na)=0$ for all $n\ge0$, then the moments $\int_0^{\infty} g(t) e^{-nat}\,dt$ vanish; by 11.23 applied to $g$ on $[0,\infty)$ via a compactification and density of polynomials in $e^{-at}$, we get $g\equiv0$, hence $f\equiv0$ on $(0,\infty)$.
\item If $\mathcal L(f)=\mathcal L(h)$ on a right half-plane, their inverse Laplace transforms coincide (11.39), so $f=h$ for $t>0$.
\end{enumerate}\qed


\begin{problembox}[11.39: Inversion Formula for Laplace Transforms]
\begin{problemstatement}
Let $F(z) = \int_0^\infty e^{-zt} f(t) \, dt$ for $z = x + iy$, $x > c > 0$. Let $t$ be a point at which $f$ satisfies one of the "local" conditions (a) or (b) of the Fourier integral theorem (Theorem 11.18). Prove that for each $a > c$, we have
\[
\frac{f(t+) + f(t-)}{2} = \frac{1}{2\pi} \lim_{T \to +\infty} \int_{-T}^T e^{(a + iv)t} F(a + iv) \, dv.
\]
This is called the inversion formula for Laplace transforms. The limit on the right is usually evaluated with the help of residue calculus, as described in Section 16.26.
\begin{itemize}
\item \textit{Hint:} Let $g(t) = e^{-at} f(t)$ for $t > 0$, $g(t) = 0$ for $t < 0$, and apply Theorem 11.19 to $g$.
\end{itemize}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Let $g(t)=e^{-at}f(t)$ for $t>0$ and $g(t)=0$ for $t<0$, then apply the Fourier inversion formula (Theorem 11.19) to $g$ at a point where $f$ satisfies a local condition.

\bigskip\noindent\textbf{Solution:}
Let $g(t)=e^{-at}f(t)$ for $t>0$, $g(t)=0$ for $t<0$. Its Fourier transform is $\widehat g(v)=F(a+iv)$. Apply the Fourier inversion formula (Theorem 11.19) at a point where $f$ satisfies a local condition to get
\[\frac{f(t+)+f(t-)}{2}=\frac{1}{2\pi}\lim_{T\to\infty}\int_{-T}^T e^{(a+iv)t}F(a+iv)\,dv.\]

\begin{techniquessection}[Solving and Proving Techniques]

\subsection*{Working with Orthogonal Systems}
\begin{itemize}
\item Use the definition of orthonormality: $\langle \varphi_m, \varphi_n \rangle = \delta_{mn}$
\item Apply trigonometric identities to compute inner products of trigonometric functions
\item Use the fact that orthonormal systems are linearly independent
\item Apply the Gram-Schmidt process to construct orthogonal systems from linearly independent ones
\item Use the fact that orthogonal systems can be used for approximation
\end{itemize}

\subsection*{Proving Linear Independence}
\begin{itemize}
\item Use the orthonormality property to take inner products with basis functions
\item Show that if $\sum c_k \varphi_k = 0$, then all coefficients $c_k = 0$
\item Apply the fact that orthonormal systems are automatically linearly independent
\item Use the fact that linear independence is preserved under orthogonalization
\item Apply the fact that infinite orthonormal systems have linearly independent finite subsets
\end{itemize}

\subsection*{Working with Fourier Series}
\begin{itemize}
\item Use the fact that Fourier coefficients are given by inner products: $c_n = \langle f, \varphi_n \rangle$
\item Apply Parseval's identity: $\|f\|^2 = \sum |c_n|^2$
\item Use the fact that Fourier series converge in $L^2$ for square-integrable functions
\item Apply the fact that continuous periodic functions can be approximated by trigonometric polynomials
\item Use the fact that Fourier series preserve periodicity
\end{itemize}

\subsection*{Proving Convergence of Fourier Series}
\begin{itemize}
\item Use the fact that Fourier series converge in $L^2$ for square-integrable functions
\item Apply the fact that uniform convergence implies pointwise convergence
\item Use the fact that continuous functions can be approximated by step functions
\item Apply the fact that Fourier coefficients depend continuously on the function in $L^2$
\item Use the fact that Riemann integrable functions can be approximated by continuous functions
\end{itemize}

\subsection*{Working with Fourier Transforms}
\begin{itemize}
\item Use the definition: $\widehat{f}(\xi) = \int_{-\infty}^{\infty} f(x) e^{-i\xi x} \, dx$
\item Apply the inversion formula: $f(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \widehat{f}(\xi) e^{i\xi x} \, d\xi$
\item Use the fact that Fourier transforms convert convolution to multiplication
\item Apply the fact that Fourier transforms preserve $L^2$ norms (Plancherel's theorem)
\item Use the fact that smooth functions have rapidly decaying Fourier transforms
\end{itemize}

\subsection*{Working with Laplace Transforms}
\begin{itemize}
\item Use the definition: $F(z) = \int_0^{\infty} e^{-zt} f(t) \, dt$
\item Apply the convolution theorem: $\mathcal{L}(f * g) = \mathcal{L}(f) \cdot \mathcal{L}(g)$
\item Use the fact that Laplace transforms are linear
\item Apply the inversion formula using residue calculus
\item Use the fact that Laplace transforms can be used to solve differential equations
\end{itemize}
\end{techniquessection}

